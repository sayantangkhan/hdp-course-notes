\documentclass[11pt]{article}

%%% The preamble loads packages, theorem styles, and macros %%%%%%%%%%%%%%
\input{CourseNotesPreamble}

\title{Math 626: High Dimensional Probability}
\author{Taught by Mark Rudelson \\ Lecture notes by Sayantan Khan}
%\email{schommerpries.chris.math@gmail.com}
%\address{Department of Mathematics \\
%Harvard University \\
%1 Oxford St. \\
%Cambridge, MA 02138}
\date{\today}

\begin{document}


\maketitle
\tableofcontents

\section{Introduction}
\label{sec:introduction}

High dimensional probability is the study of random variables taking values in $\RR^n$ for large but fixed values of $n$.
While this area has always been studied by classical probability theorists, it has also attracted attention from computer scientists, especially since the design and analysis of fast probabilistic algorithms requires tools and theorems from this field.
A classical result that arises from pure mathematics, but has several real life applications is Dvoretzky's theorem, whose statement does not involve any probability at all, and yet the proof uses concentration inequalities.
\begin{theorem}[Dvoretzky's theorem \cite{dvoredsky1961some}]
  Let $X$ be an $n$-dimensional normed vector space. Then for any $\varepsilon > 0$, there exists a constant $c(\varepsilon) > 0$, and a linear subspace $E$ of $X$ such that, $\dim(E) \geq c(\varepsilon) \log(n)$, and for all $v \in E$, the following inequality relating the ambient norm and the Euclidean norm on $E$.
  \begin{align*}
    (1-\varepsilon) \norm{v}_2 \leq \frac{\norm{v}_X}{M(X)} \leq (1+\varepsilon)\norm{x}_2
  \end{align*}
  Where $M(X)$ is a scaling factor that depends only on $X$, and $\norm{\cdot}_X$ and $\norm{\cdot}_2$ are the ambient and Euclidean norm on $E$.
\end{theorem}

\begin{proof}[Idea of proof]
  Pick a random subspace, and then show that with very high probability, the given inequality holds.
  We will prove the result in full detail later in the course.
\end{proof}

\begin{remark}
  \label{rem1}
  When the norm on $X$ is the $\ell^1$ norm, the lower bound on the dimension of $E$ can be improved to be linear in $c(\varepsilon) n$.
\end{remark}

\paragraph{A computer science application of Dvoretzky's theorem}

Consider a subset $S$ of $\ell_2^N$, given by inequalities involving the norms of elements in $\ell_2^N$.
Suppose that we are required to optimize a linear function $f$ on the set $S$.
Since $S$ is given by inequalities involve the $\ell^2$-norm, it will be and intersection of interiors of ellipsoids, and consequently, optimizing $f$ will be computationally expensive.
But we can get around the computational expense by embedding $\ell_2^N$ into $\ell_1^M$, where $M$ is $O(N)$, by Remark \ref{rem1}.
Since this embedding does not distort distances too much, we can replace $S$ with a nearby polytope, given by inequalities involving the $\ell^1$ norm instead.
Optimizing a linear function on a polytope is computationally much easier, thanks to linear programming.

\paragraph{Empirical covariance estimation}

Let $X$ be an $\RR^n$-valued random variable, and let $\E(X) = 0$.
The covariance of $X$ is $\E(X^{\top}X)$, and will be denoted by $A$.
Let $\left\{ X_1, X_2, \ldots, X_m \right\}$ be i.i.d. samples of $X$.
We define the sample covariance $A_m$ in the following manner.
\begin{align*}
  A_n = \frac{\sum_{i=1}^m X_i^{\top}X_i}{m}
\end{align*}
As $m$ tends to infinity, the sample covariance $A_m$ will approach the true covariance, as we would expect the law of large numbers to predict.
A harder, and more interesting question is to determine how many samples do we need to take to be within some threshold of the true covariance with high probability.

Just like in the scalar setting, one answers the question by proving appropriate concentration inequalities for matrix valued random variables.
Here is the most general set up: Let $A_m$ and $A$ be matrices mapping some normed space $X$ to some other normed space $Y$.
We define the distance between $A_m$ and $A$ using the operator norm.
\begin{align*}
  \norm{A_m - A}_{\mathrm{op}} &= \max_{\norm{v}_x \leq 1} \norm{(A_m-A)v}_Y \\
                               &= \max_{\norm{v}_x \leq 1} \max_{\norm{w}_{Y^{\ast}} \leq 1} w\left( (A_m - A)v \right) \\
                               &= \max_{\substack{(v, w) \in X \times Y^{\ast} \\ \norm{v} \leq 1 \\ \norm{w} \leq 1}} w((A_m - A)v)
\end{align*}
We can consider $w((A_m -A)v)$ to be a family of scalar random variables parameterized by points in $X \times Y^{\ast}$, i.e. a random process $V_u$ parameterized by $u \in X \times Y^{\ast}$.
This leads to the following two questions.
\begin{enumerate}[(i)]
\item How to bound $\E \max V_u$.
\item How to bound $\P(\left| \max V_u - \E \max V_u \right| \geq t)$.
\end{enumerate}
It turns out one can often answer (ii) without answering (i), which may seem surprising given that the most elementary concentration inequalities involve moment bounds (i.e. Markov's inequality).
The starting point in answering (ii) is understanding \emph{concentration of measure}.

\section{Concentration of measure}
\label{sec:conc-meas}

Concentration of measure was originally observed LÃ©vy, but first used by Milman in the early 1970s.
Roughly speaking, concentration of measure is the following phenomenon: suppose $(X, d, \P)$ is a metric space endowed with a probability measure and $f: X \to \RR$ is a ``nice'' function.
Then the value of $f$ is essentially constant, i.e. there exists some constant $M(f)$ such that for small enough $\varepsilon$, the following probability bound holds.
\begin{align*}
  \P(|f(x) - M(f)| < \varepsilon) \ll 1
\end{align*}
Usually by nice, we will mean $1$-Lipschitz, although similar results hold for a more general class of functions like convex or quasi-convex functions.
We begin with the simplest version of measure concentration.

\subsection{Concentration for linear functions}
\label{sec:conc-line-funct}

Let the metric space $X$ in this setting be $\R^n$ for some fixed $n$, and let $\{X_1, \ldots, X_n \}$ be i.i.d scalar random variables.
Let $f: \R^n \to \R$ be a linear function, given by taking inner product with some vector $\mathbf{a}$.
We define $Y$ to be $\sum_{i=1}^n a_i X_i$.
We will prove concentration inequalities for $Y$ by imposing conditions on $X_i$: one such condition is requiring $X_i$ to be \emph{subgaussian}.

\subsubsection{Subgaussian random variables}
\label{sec:subg-rand-vari}

\begin{definition}[Subgaussian decay]
  A random variable $X$ is said to be $\sigma$-subgaussian if there exists a constant $C > 0$, such that for all $t > 0$, the following inequality holds.
  \begin{align*}
    \P(|X| > t) \leq C \exp\left( -\frac{1}{2} \left( \frac{t}{\sigma} \right)^2 \right)
  \end{align*}
\end{definition}
\begin{remark}
  The constant $\sigma$ is often referred to as the variance proxy of the subgaussian random variable.
\end{remark}

\begin{example}
  The following random variables are examples of subgaussian random variables.
  \begin{enumerate}[(i)]
  \item Normal random variables
  \item Bounded random variables
  \end{enumerate}
\end{example}

\begin{lemma}
  Let $X$ be a random variable. Then the following are equivalent:
  \begin{enumerate}[(i)]
  \item For all $t> 0$, $\P(|X| > t) \leq C \exp\left( -\frac{1}{2} \left( \frac{t}{\sigma} \right)^2 \right)$ (definition of subgaussian random variables).
  \item There exists $a > 0$ such that $\E(\exp(aX^2)) < \infty$ ($\psi_2$ condition).
  \item There exist $C^{\prime}$ and $b$ such that for all $\lambda \in \R$, $\E(\exp(\lambda X)) \leq C^{\prime} \exp(b \lambda^2)$ (Laplace transform condition).
  \item There exists $K$ such that for all $p \geq 1$, $\E(X^p)^{\frac{1}{p}} \leq K \sqrt{p}$ (moment condition).
  \end{enumerate}
  Moreover, if $\E(X) = 0$, then the constant $C^{\prime}$ in the Laplace transform condition can be taken to be equal to $1$.
\end{lemma}

\begin{proof}
  \begin{description}
  \item[$(i) \implies (ii)$] Using Fubini's theorem and a change of variables, we can express $\E(aX^2)$ as an integral involving tail bounds.
    \begin{align*}
      \E(aX^2) &= 1 + \int_0^{\infty} 2 a t e^{at^2} \cdot \P\left( |X| > t \right) \dd t \\
      &\leq 1 + \int_0^\infty 2Cat\exp\left( at^2 - \frac{1}{2}\left( \frac{t}{\sigma}  \right)^2 \right) \dd t
    \end{align*}
    Clearly, picking a value of $a$ smaller than $\frac{1}{2\sigma^2}$ will make the integral converge.
  \item[$(ii) \implies (iii)$] Since we want to estimate the expectation of $\exp(\lambda X)$, we multiply and divide by $\exp(aX^2)$ and complete the square.
    \begin{align*}
      \exp(\lambda X) = \exp\left(aX^2\right) \cdot \exp\left( \frac{\lambda^2}{4a} \right) \cdot \exp\left( - \left( \sqrt{a}X + \frac{\lambda}{2\sqrt{a}} \right)^2 \right)
    \end{align*}
    Note that the third term in the product is always less than $1$. We now take the expectation of the right hand side.
    \begin{align*}
      \E(\exp(\lambda X)) &= \E\left( \exp\left(aX^2\right) \cdot \exp\left( \frac{\lambda^2}{4a} \right) \cdot \exp\left( - \left( \sqrt{a}X + \frac{\lambda}{2\sqrt{a}} \right)^2 \right) \right) \\
      &\leq \exp\left( \frac{\lambda^2}{4a} \right) \E\left( \exp(aX^2) \right)
    \end{align*}
    Setting $b= \frac{1}{4a}$ and $C^{\prime} = \E(\exp(aX^2))$, we get the result.
  \item[$(iii) \implies (iv)$] We begin by getting a crude estimate for $\E(X^p)$ using the infinite series for $\exp(\lambda X)$.
    \begin{align*}
      \E(X^p) &\leq \frac{p!}{\lambda^p} \E(\exp(\lambda X)) \\
              &= \frac{C^{\prime}p! \exp(b\lambda^2)}{\lambda^p}
    \end{align*}
    Note that this inequality works for all values of $\lambda$, but to get the best inequality, we minimize the right hand side by varying $\lambda$ over $\R$.
    The minimum is attained when $\lambda = \frac{\sqrt{p}}{2b}$: plugging that into the right hand side, and taking $p$\textsuperscript{th} roots gives us the following.
    \begin{align*}
      \E(X^p)^{\frac{1}{p}} \leq C^{\prime \prime} \frac{(p!)^{\frac{1}{p}}}{\sqrt{p}}
    \end{align*}
    Here, we have absorbed all the constants into $C^{\prime \prime}$.
    Using Stirling's approximation, the numerator is bounded above by $p$, giving us the inequality we want.
  \item[$(iv) \implies (i)$] We rewrite the event $|X| > t$ in the following manner.
    \begin{align*}
      \P(|X| > t) &= \P(\exp(\lambda X^2) > \exp(\lambda t^2)) \\
                  &\leq \exp(-\lambda t^2) \E(\exp(\lambda X^2))
    \end{align*}
    Here, $\lambda$ is a positive real number that we will specify later, and the inequality comes from Markov's inequality.
    Of course, we do not a priori know that $\E(\lambda X^2)$ is finite, but we will pick a $\lambda$ small enough such that it is.
    Using Fubini's theorem, we can express $\E(\exp(\lambda X^2))$ in the following manner.
    \begin{align*}
      \E(\exp(\lambda X^2)) = 1 + \frac{\lambda \E(X^2)}{1!} + \frac{\lambda^2 \E(X^4)}{2!} + \cdots
    \end{align*}
    Using the bound on the moments of $X$ and Stirling's approximation, we get the following inequality.
    \begin{align*}
      \E(\exp(\lambda X^2)) &\leq \sum_{p=0}^{\infty} \frac{(2\lambda K^2p)^p}{p!} \\
                            &\leq \sum_{p=0}^{\infty} \frac{(2\lambda e K^2 p)^p}{\sqrt{2\pi p} p ^p}
    \end{align*}
    If we pick $\lambda$ to be small enough that $2e \lambda K^2$ is much smaller than $1$, then the infinite sum converges, and the expectation is finite.
    Setting $\frac{1}{2\sigma^2}$ to be equal to $\lambda$ gives us the result.
  \end{description}
  We now show that the constant in the Laplace transform condition can be set to be $1$ when $\E(X) = 0$.
  To do so, we recall the $\psi_2$ and the Laplace transform condition, i.e. there exist constants $a$, $C^{\prime}$ and $b$ such that the following two inequalities hold for all $\lambda \in \R$.
  \begin{align}
    \label{eq:1}
    \E(\exp(aX^2)) &< \infty \\
    \E(\exp(\lambda X)) & \leq C \exp(b\lambda^2)
  \end{align}
  Suppose now that $\lambda^2 > 2a$.
  By the Laplace transform condition, we have the following inequality.
  \begin{align*}
    \E(\exp(2aX)) &\leq C^{\prime} \exp(4ba^2) \\
                  &= \exp(4a^2 b^{\prime})
  \end{align*}
  Where $b^{\prime}$ is $b + \frac{\log(C^{\prime})}{4a^2}$.
  Since $b^{\prime}$ decreases as $a$ increases, for any $\lambda^2 > 2a$, $\E(\exp(aX^2))$ will be less than $\exp(b^{\prime} \lambda^2)$.

  Now suppose that $\lambda^2 < 2a$.
  We begin by considering the special case where $X$ is a symmetric random variable.
  By symmetry of $X$, we have the following inequality.
  \begin{align*}
    \exp(\lambda X) &= \frac{\exp(\lambda X) + \exp(-\lambda X)}{2} \\
            &\leq \exp\left( \frac{\lambda^2 X^2}{2} \right)
  \end{align*}
  Taking expectations on both sides gives us the following.
  \begin{align*}
    \E(\exp(\lambda X)) \leq \E\left(\exp\left( \frac{\lambda^2}{2} X^2 \right)\right)
  \end{align*}
  Since $\lambda^2 < 2a$, $\frac{2a}{\lambda^2}$ is greater than $1$, and we can use HÃ¶lder's inequality to bound the right hand term.
  \begin{align*}
    \E\left(\exp\left( \frac{\lambda^2}{2} X^2 \right)\right) \leq
    \left( \E\left(\exp\left(   aX^2   \right) \right) \right)^{\frac{\lambda^2}{2a}}
  \end{align*}
  Since $\E(\exp(aX^2))$ is a finite constant, the right hand side is $\exp(b^{\prime \prime} \lambda^2)$ for some constant $b^{\prime \prime}$.

  Now suppose $X$ is not symmetric.
  Let $X^{\prime}$ be an identical independent copy of $X$.
  Since $\E(X^{\prime})$ is $0$, we have the following equality.
  \begin{align*}
    \E(\exp(\lambda X)) = \E(\exp(\lambda (X - \E(X^{\prime}))))
  \end{align*}
  Since $\exp$ is a convex function, we can pull out the inner expectation, using Jensen's inequality.
  \begin{align*}
    \E(\exp(\lambda X)) \leq \exp(\lambda(X - X^{\prime}))
  \end{align*}
  Since $X - X^{\prime}$ is symmetric, the result follows from the previous part, and the proof is complete.
\end{proof}
We now explain why care so much about the constant in the Laplace transform condition.

\begin{theorem}[Hoeffding-Chernoff-Azuma inequality]
  \label{thm:hoeffding}
  Let $\{X_1, \ldots, X_n\}$ be i.i.d. subgaussian random variables with mean $0$.
  Then for any $(a_1, \ldots , a_n) \in \R^n$ and any $t > 0$, the following probability bound holds.
  \begin{align*}
    \P\left( \left| \sum_{i=1}^n a_i X_i \right| > t \right) \leq C \exp\left( -c \frac{t^2}{\norm{\mathbf{a}}_2} \right)
  \end{align*}
  Where $C$ and $c$ are some absolute constants.
\end{theorem}
\begin{proof}
  Without loss of generality, we can assume $\norm{\mathbf{a}}_2 = 1$.
  It will suffice to show that the sum of subgaussian random variables is subgaussian.
  We will show it verifying the Laplace transform condition.
  Let $\lambda \in \R$, and let $Y = \sum_{i=1}^n a_iX_i$.
  We compute $\E(\exp(\lambda Y))$.
  \begin{align*}
    \E(\exp(\lambda Y)) &= \prod_{i=1}^n \E(\exp(\lambda a_i X_i)) \\
                        &\leq \prod_{i=1}^n \E(\exp(b \lambda^2 a_i^2)) \\
                        &= \E(\exp(b\lambda^2))
  \end{align*}
  This proves the result. Note that having the Laplace transform coefficient equal to $1$ helped, because if the coefficient $C$ was greater than $1$, then we would pick up a constant of $C^n$, which would be very large for large values of $n$.
\end{proof}

To see how this inequality is used in practice, consider the simplest possible example of a subgaussian random variable, a random variable that takes values $1$ and $-1$ with probability $\frac{1}{2}$.
% A Rademacher random variable is a random variable that takes values $1$ and $-1$ with probability $\frac{1}{2}$.

Let's recall some elementary facts about Fourier analysis before stating the example.
Let $L^2([0,1])$ be the space of complex $L^2$ functions on $[0,1]$ and let $\{e_n\}_{n \in \Z}$ be the standard Fourier basis, i.e. $e_n(t) = \exp(2\pi i nt)$.
Since $\{e_n\}$ forms an orthonormal basis, any function $f \in L^2([0,1])$ can be decomposed into its Fourier series.
\begin{align*}
  f = \sum_{n \in \Z} \widehat{f}(n) e_n
\end{align*}
The Fourier coefficient $\widehat{f}(n)$ is given by $\int_0^1 f(t) \overline{e_n(t)} \dd t$.
The map sending $f$ to its Fourier coefficients is a linear isometry from $L^2([0,1])$ to $\ell^2(\Z)$.
For most sequences $\{\widehat{f}(n)\}$ in $\ell^2(\Z)$, the associated function $f$ will not be continuous, but we will show that under reasonably mild conditions on $\{\widehat{f}(n)\}$, $f$ can be made to be continuous.

Let $\varepsilon_n \in \{-1, 1\}$ for $n \in \Z$, and let $\varepsilon = \{\varepsilon_n\}_{n \in \Z}$.
For any $f \in L^2([0,1])$ and any such $\\varepsilon$, define $f_{\varepsilon}$ to be the following function.
\begin{align*}
  f_{\varepsilon} = \sum_{n \in Z} \varepsilon_n \widehat{f}(n) e_n
\end{align*}
We then have the following theorem.
\begin{theorem}
  \label{thm:continuous-l2-function}
  Let $f$ be a function in $L^2([0,1])$ whose Fourier coefficients satisfy the following inequality.
  \begin{align*}
    % \label{eq:convergence-condition-example}
    \sum_{n \in \Z} \left( \log(|n| + 1) \right)^3 \left| \widehat{f}(n) \right|^2 < \infty
  \end{align*}
  Let $\{\varepsilon_n\}$ be a sequence of i.i.d random variables taking values in $1$ and $-1$ with probability $\frac{1}{2}$.
  Then $f_{\varepsilon}$ is a continuous function with probability $1$.
\end{theorem}

To prove the theorem, we will need several lemmas.

\begin{lemma}
  \label{lem:main-technical-lemma}
  Let $N \in \N$.
  Then for any $\{a_n\}_{n=-N}^N$, the following probability bound holds.
  \begin{align*}
    \P\left( \norm{ \sum_{n=-N}^N \varepsilon_n a_n e_n }_{\infty} >  C \sqrt{\log(N)} \left( \sum_{n=-N}^N |a_n|^2 \right)^{\frac{1}{2}} \right) \leq \frac{1}{N^2}
  \end{align*}
  Here the constant $C$ is absolute, i.e. independent of $N$.
\end{lemma}
\begin{proof}
  The first step is to consider the maximum, not over all of $[0,1]$, but over the points $\left\{ \frac{j}{N^2} \right\}_{j = 0}^N$.
  It will suffice to bound the probability for any given point by $\frac{1}{N^4}$, and then use the union bound to get the desired inequality.
  Since $\varepsilon_n$ is a subgaussian random variable with mean $0$, by Hoeffding-Chernoff-Azuma inequality, we get the following bound for any $t \in [0,1]$.
  \begin{align*}
    \P\left( \norm{ \sum_{n=-N}^N \varepsilon_n a_n e_n }_{\infty} >  C \sqrt{\log(n)} \left( \sum_{n=-N}^N |a_n|^2 \right)^{\frac{1}{2}} \right) \leq C^{\prime} \exp(-cC^2 \log(n))
  \end{align*}
  We can pick a $C$ large enough so that the right hand side is bounded above by $\frac{1}{N^4}$, and that concludes the first step after we use the union bound.
  Note that in order to do this, we really needed something stronger than Chebyshev's inequality, since we needed an upper bound that can be made smaller than $\frac{1}{N^4}$.

  The next step is to extend the argument to all of $[0,1]$.
  The key trick here will be to estimate the maximum value of trigonometric polynomials away from points of the form $\frac{j}{N^2}$ using the maximum we derived in step $1$.
  Bernstein's inequality for trigonometric polynomial helps in this regard: given a trigonometric polynomial $p$ of degree $n$, the following inequality relates the $\norm{\cdot}_{\infty}$-norm of $p^{\prime}$ and $p$.
  \begin{align*}
    \norm{p^{\prime}}_{\infty} \leq n \norm{p}_{\infty}
  \end{align*}
  Let $V$ be the maximum value of the trigonometric polynomial $p = \sum_{n=-N}^N \varepsilon_n a_n e_n$ achieves on points of the form $\frac{j}{N^2}$, and let $W$ be the maximum value over all of $[0,1]$, and say it is achieved at some point $t$, and let $s$ be the closest point of the form $\frac{j}{N^2}$.
  Then, by the mean value theorem, we get the following relation between $V$ and $W$.
  \begin{align*}
    W &\leq V + \norm{p^{\prime}}_{\infty} |t-s| \\
      &\leq V + \frac{N \norm{p}_{\infty}}{N^2} \\
      &= V + \frac{W}{N}
  \end{align*}
  This means for $N > 1$, $W \leq 2V$, and this proves the lemma.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:continuous-l2-function}]
  For $M \in \N$, define the function $f_{M, \varepsilon}$ in the following manner.
  \begin{align*}
    f_{M, \varepsilon} = \sum_{2^M \leq |n| < 2^{M+1}} \varepsilon_n \widehat{f}(n) e_n
  \end{align*}
  We now use Lemma \ref{lem:main-technical-lemma} with $N = 2^{M+1}$, $a_n = 0$ for $|n| < 2^M$, and $a_n = \widehat{f}(n)$ for $2^M \leq n < 2^{M+1}$.
  \begin{align*}
    \P\left( \norm{f_{M,\varepsilon}}_{\infty} > C \sqrt{M} \norm{f_{M,\varepsilon}}_2 \right) < \frac{1}{2^{2(M+1)}}
  \end{align*}
  % \confused{It seems something is going wrong here because the sum of probability upper bound converges too easily, which means one might be making a mistake, or strengthen the result significantly.}
  By the Borel-Cantelli lemma, for almost every $\varepsilon$, $\norm{f_{M, \varepsilon}}_{\infty}$ eventually becomes smaller than $C\sqrt{M} \norm{f_{M,\varepsilon}}_2$.

  Pick $\varepsilon$ to be one of the instances where the above described situation does happen.
  We have that $f$ is an infinite sum of continuous functions.
  \begin{align*}
    f = \varepsilon_0 \widehat{f}(0) e_0 + \sum_{M=0}^{\infty} f_{M, \varepsilon}
  \end{align*}
  This will converge to a continuous function if the sequence of partial sums is uniformly Cauchy.
  To see that is indeed the case, pick $K_1$ and $K_2$ larger than the threshold $M$ after which $\norm{f_{M, \varepsilon}} < C\sqrt{M}$.
  \begin{align*}
    \norm{f_{K_2, \varepsilon} - f_{K_1, \varepsilon}}_{\infty} &\leq \sum_{M=K_1}^{K_2} \norm{f_{M, \varepsilon}}_{\infty} \\
                                                                &\leq \sum_{M=K_1}^\infty C \sqrt{M} \norm{f_{M, \varepsilon}}_2 \\
                                                                &\leq C \left( \sum_{M=K_1}^{\infty} \left( \frac{1}{M} \right)^2 \right)^{\frac{1}{2}} \left( \sum_{M=K_1}^{\infty} M^3 \norm{f_{M, \varepsilon}}_2^2 \right)^{\frac{1}{2}} \\
                                                                &\leq \frac{C^{\prime}}{K_1} \left( \sum_{n \in \Z} (C^{\prime \prime} \log(|n| + 1))^3 \widehat{f}(n)^2 \right) \\
    &\leq \frac{C^{\prime\prime\prime}}{K_1}
  \end{align*}
  The upper bound goes to $0$ as $K_1$ goes to $\infty$, which shows the sequence is uniformly Cauchy, and thus the limit is a continuous function.
\end{proof}

We now prove a moment bound for sums of subgaussian random variables.
\begin{theorem}[Khintchine's inequality \cite{khintchine1923dyadische}]
  Let $\{X_1, \ldots, X_n\}$ be i.i.d subgaussian random variables with $\E(X_i) = 0$ and $\E(X_i^2) = 1$.
  Then for any $p \in [1, \infty)$, there exist constants $A_p$ and $B_p$ greater than $0$ such that for any vector $\mathbf{a} \in \R^n$, the following moment bound holds.
  \begin{align*}
    A_p \norm{\mathbf{a}}_2 \leq \left( \E \left| \sum_{i=1}^n a_i X_i \right|^p \right)^{\frac{1}{p}} \leq  B_p \norm{\mathbf{a}}_2
  \end{align*}
\end{theorem}
\begin{proof}
  Consider first the case where $p > 2$.
  Then, using HÃ¶lder's inequality for the convex function $x \mapsto x^{\frac{p}{2}}$, we get the following.
  \begin{align*}
\left( \E \left| \sum_{i=1}^n a_i X_i \right|^2 \right)^{\frac{1}{2}} \leq \left( \E \left| \sum_{i=1}^n a_i X_i \right|^p \right)^{\frac{1}{p}}
  \end{align*}
  This shows that for $p > 2$, we can set $A_p = 1$.
  To get $B_p$, we use the moment condition on subgaussian random variables.
  Since the sum of subgaussian random variables is subgaussian, we have that $Y = \sum_{i=1}^n a_i X_i$ is subgaussian, and thus satisfies the moment bound.
  We have seen that the absolute constant $K$ will only depend on $\norm{\mathbf{a}}_2$, giving us the upper bound.
  \begin{align*}
    \E(|Y|^p)^{\frac{1}{p}} \leq K \sqrt{p} \norm{\mathbf{a}}_2
  \end{align*}
  Setting $B_p = K\sqrt{p}$ proves the result in this case.

  For $p < 2$, it will suffice to prove it for $p = 1$, since the $p$\textsuperscript{th} moment of $|Y|$ is an increasing function of $Y$ and bounded above by $\norm{\mathbf{a}}_2$ by HÃ¶lder's inequality, which means we can set $B_p = 1$ (or the previous argument will also work, but $B_p = 1$ is better than $B_p = K\sqrt{p}$).
  Thus we just need to show the lower bound for $p = 1$.
  In this case, the inequality follows from Cauchy-Schwartz.
  \begin{align*}
    \E(|Y|^2) &\leq \sqrt{\E(|Y|) \E(|Y|^3)}
  \end{align*}
  Using Khintchine's inequality for $p=3$, we deal with $\E(|Y|^3) \leq B_3 \norm{\mathbf{a}}_2$.
  Squaring both sides, we see that $A_1 = B_3^{-3}$ works, and the proof is complete.
\end{proof}
There is a far reaching generalization of Khintchine's inequality, due to Kahane.
\begin{theorem}[Kahane inequality \cite{kahane1964sommes}]
  Let $X$ be a normed vector space, and let $\{\varepsilon_1, \ldots, \varepsilon_n\}$ be i.i.d. Rademacher random variables.
  For any $p \in [1, \infty)$, there exists $A_p$ and $B_p$ greater than $0$ such that for any $\{a_1, \ldots, a_n\}$ in $X$, the following holds.
  \begin{align*}
    A_p \left( \E \norm{\sum_{j=1}^n \varepsilon_j a_j}^2_X \right)^{\frac{1}{2}} \leq \left( \E \norm{\sum_{j=1}^n \varepsilon_j a_j}^p_X \right)^{\frac{1}{p}} \leq B_p \left( \E \norm{\sum_{j=1}^n \varepsilon_j a_j}^2_X \right)^{\frac{1}{2}}
  \end{align*}
\end{theorem}
The proof of this inequality requires more machinery than the previous result, so we'll defer the proof until we have developed the required tools.

Recall that we got strong tail decay for bounded random variables using Hoeffding-Chernoff-Azuma inequality, since they're subgaussian, but it turns out, we can do much better than that using boundedness.

\begin{theorem}[Bennett's inequality]
  \label{thm:bennett}
  Let $\{X_1, \ldots, X_n\}$ be i.i.d random variables satisfying the following properties.
  \begin{enumerate}[(i)]
  \item $\norm{X_j}_{\infty} \leq 1$.
  \item $\E(X_j) = 0$ and $\E(X_j^2) = \delta$.
  \end{enumerate}
  Then for any $\mathbf{a} \in \R^n$, we have the following tail decay estimate.
  \begin{align*}
    \P\left( \left| \sum_{j=1}^n a_j X_j \right| > t \right) \leq
    \begin{cases}
      2 \exp \left(- \frac{t^2}{2e\delta \norm{\mathbf{a}}_2^2} \right)\text{ for $t \leq t_{\ast}$} \\
      2 \exp \left( - \frac{t}{4 \norm{\mathbf{a}}_{\infty}} \cdot \log\left( \frac{t \norm{\mathbf{a}}_{\infty}}{\delta \norm{\mathbf{a}}_2^2} \right) \right)\text{ for $t > t_{\ast}$}
    \end{cases}
  \end{align*}
  Here $t_{\ast} = e\delta \norm{\mathbf{a}}_2^2$.
\end{theorem}
Before we start to prove the theorem, let's show why the described tail bound is a very natural upper bound to consider.
Consider $\mathbf{a} = (1, 1, \ldots, 1)$.
Then $\sum a_j X_j$ is approximately $\cN(0, \delta \norm{\mathbf{a}}^2_2) = \cN(0, \delta n)$.
The tail should then behave something like $\exp \left( - \frac{t^2}{2\delta n} \right)$, which is precisely the first case in the upper bound.
If $\delta$ is small, i.e. $\delta n$ is bounded above by some constant $\lambda$, then the central limit theorem asymptotic does not apply, but rather the Poisson limit theorem asymptotic applies.
\begin{align*}
  \P\left( \sum_{j=1}^n a_j X_j > t \right) &\sim \sum_{j=t}^{\infty} e^{-\lambda} \frac{\lambda^j}{j!} \\
                                            &\sim e^{-\lambda} \frac{\lambda^j}{j!} \\
                                            &\sim \exp\left( -t \log\left( \frac{t}{e\lambda} \right) \right)
\end{align*}
Contrast this with the second case of the tail in Bennett's inequality.

\begin{proof}[Proof of Theorem \ref{thm:bennett}]
  Without loss of generality, assume that $\norm{\mathbf{a}}_{\infty} = 1$.
  Set $Y = \sum_{j=1}^n a_j X_j$, and let $\lambda > 0$.
  We then estimate the Laplace transform of $Y$.
  \begin{align*}
    \E\left( \exp(\lambda Y) \right) &= \prod_{j=1}^n \E\left(\exp\left(\lambda a_j X_j \right) \right)
  \end{align*}
  We use an elementary inequality to estimate the Laplace transform.
  \begin{align*}
    e^x \leq 1 + x + \frac{x^2}{2} e^{|x|}
  \end{align*}
  Thus, we have the following.
  \begin{align*}
    \E\left( \exp\left( \lambda a_j X_j \right) \right) &\leq \E\left(1 + \lambda a_j X_j + \frac{\lambda^2 a_j^2 X_j^2}{2} \exp\left( \left| \lambda a_j X_j \right| \right)\right) \\
    &\leq 1 + 0 + \frac{\lambda^2 a_j^2 \delta}{2} e^{\lambda}
  \end{align*}
  Putting all of it back together, we have the following inequality.
  \begin{align*}
    \E\left( \exp\left( \lambda Y \right) \right) &\leq \prod_{j=1}^n \left( 1 + \frac{\lambda^2 a_j^2 \delta e^{\lambda}}{2} \right) \\
                                                  &\leq \prod_{j=1}^n \exp\left( \frac{\lambda^2 a_j^2 \delta e^{\lambda}}{2} \right) \\
                                                  &= \exp\left( \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)
  \end{align*}
  Now that we have the Laplace transform estimate, we can use it to estimate tail probabilities.
  \begin{align*}
    \P(Y > t) &= \P(\exp(\lambda Y) > e^{\lambda t}) \\
              &\leq \frac{\exp\left( \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)}{e^{\lambda t}} \\
              &= \exp\left( -\lambda t + \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)
  \end{align*}

  The next step is to optimize this inequality as we vary $\lambda$.
  We begin by optimizing in the region $\lambda \leq 1$.
  In this case, the optimum $\lambda$ is $\frac{t}{e\delta \norm{\mathbf{a}_2^2}}$.
  Plugging in this value of $\lambda$ gives the first case of the upper bound, and this is valid for $t \leq e\delta \norm{\mathbf{a}}_2^2 = t_{\ast}$.

  In the region $\lambda > 1$, we use the inequality $\lambda \leq e^{\lambda}$.
  \begin{align*}
    \P(Y > t) &\leq \exp\left( -\lambda t + \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right) \\
              &\leq \exp\left( -\lambda t + \frac{\delta\lambda e^{2\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)
  \end{align*}
  Choose $\lambda$ such that $\lambda \norm{\mathbf{a}}_2^2 e^{2\lambda} = t$. Plugging that value in, we get the second case.

  Finally, doing this for $-t$ gives similar bounds, and we combine the two using union bound to get the claimed result. This completes the proof.
\end{proof}

The theorems in this section illustrate how strong the condition of being subgaussian is, especially when taking sums of i.i.d copies of subgaussians.
In the next section, we will investigate another similar tail decay condition.

\subsubsection{Subexponential random variables}
\label{sec:subexp-rand-vari}

\begin{definition}[Subexponential random variable]
  A random variable $X$ is said to be $k$-subexponential if for all $t > 0$, the following holds.
  \begin{align*}
    \P\left( |X| > t \right) \leq 2 \exp\left(- \frac{t}{k} \right)
  \end{align*}
\end{definition}

Just like in the case of subgaussian random variables, we have a number of equivalent definitions of subexponential random variables.

\begin{lemma}
  Let $X$ be a random variable. Then the following conditions are equivalent.
  \begin{enumerate}[(i)]
  \item $X$ is $k$-subexponential.
  \item There exists a $b > 0$ such that $\E\left( \exp(b|X|) \right) \leq 2$ ($\psi_1$ condition).
  \item For all $p \geq 1$, $\E\left( |X|^p \right)^{\frac{1}{p}} \leq Cp$ (moment condition).
  \end{enumerate}
  Moreover, if $\E(X) = 0$, there exists $\lambda_0 > 0 $ such that for all $|\lambda| < \lambda_0$, $\E \left( \exp(\lambda X) \right) \leq \exp\left( \widetilde{C}\lambda^2 \right)$.
\end{lemma}

\begin{proof}
  % \begin{description}
  % \item[$(i) \implies (ii)$] Using the integrated tail probability expectation formula, we can express $\E\left( \exp(b|X|) \right)$ as the following integral.
  %   \begin{align*}
  %     \E\left( b|X| \right) &= 1 + \int_0^\infty b\exp(bt) \cdot \P\left( |X| > t \right) \dd t \\
  %                           &\leq 1 + \int_0^{\infty} b \exp\left( bt - \frac{t}{k} \right) \dd t
  %   \end{align*}
  %   Clearly, picking a small enough $b$ makes the right hand side converge to a number smaller than $2$.
  % \item[$(i) \implies (ii)$]
  % \end{description}
  The proofs of the three equivalences are similar in spirit to the versions for subgaussians, so we will skip the proof, and just prove the moreover part.

  Writing $\E(\exp(\lambda X))$ as an infinite sum of expectations, we get the following chain of inequalities for a small enough value of $\lambda$.
  \begin{align*}
    \E(\exp\left( \lambda X \right)) &= 1 + \E(\lambda X) + \sum_{j=2}^{\infty} \frac{\lambda^j \E(X^j)}{j!} \\
                                     &\leq 1 + 0 + \sum_{j=2}^{\infty} \frac{\left( \lambda C j \right)^j}{j!} \\
                                     &\leq 1 + \sum_{j=2}^{\infty} (\lambda C e)^j \\
                                     &= 1 + \frac{(\lambda C e)^2}{1 - \lambda C e}
  \end{align*}
  We get the first inequality from the moment condition, the second from Stirling's approximation, and the third follows from geometric convergence for $\lambda C e < 1$.
  Note now that if $\lambda C e < \frac{1}{2}$, we get the following inequalities.
  \begin{align*}
    1 + \frac{(\lambda C e)^2}{1 - \lambda C e} &\leq 1 + 2C^2e^2\lambda^2 \\
    &\leq \exp\left( 2C^2e^2 \lambda^2 \right)
  \end{align*}
  This proves the result.
\end{proof}

We can now prove a strong tail bound for sums of subexponential random variables like we did in Hoeffding-Chernoff-Azuma inequality.

\begin{theorem}[Bernstein's inequality]
  \label{thm:bernstein}
  Let $\{X_1, \ldots, X_n\}$ be i.i.d subexponential random variables with mean $0$, and let $\mathbf{a}$ be a vector in $\R^n$. Then the following holds.
  \begin{align*}
    \P\left( \left| \sum_{j=1}^n a_j X_j \right| > t \right) \leq
    2 \exp\left( -c \left( \frac{t^2}{\norm{\mathbf{a}}_2^2} \wedge  \frac{t}{\norm{\mathbf{a}}_\infty}\right) \right)
  \end{align*}
  Here $a \wedge b$ denotes the minimum of $a$ and $b$.
\end{theorem}
\begin{remark}
  The tail of a sum of i.i.d random variables behaves very much like the situation described above.
  When $t$ is small, the tail behave like a subgaussian, and when $t$ is large, the tail behaves like a subexponential random variable.
\end{remark}

\begin{proof}[Sketch of proof of Theorem \ref{thm:bernstein}]
  Like with all the other sum tail bounds, the proof of this theorem is via bounding the Laplace transform of the random variable.
  For small $\lambda$, we have the upper bound to be $\exp(\widetilde{C}\lambda^2)$ and then we optimize over $\lambda$, and for large $t$, we use Markov's inequality.
\end{proof}

\subsubsection{Applications of subgaussian and subexpontial random variables}
\label{sec:appl-subg-subexp}

Before we list some of the applications, we make a remark on why the conditions for subexpontial and subgaussian random variables were called $\psi_1$ and $\psi_2$ conditions respectively.
Let $\alpha$ be a number greater than $0$.
Define a function $\psi_{\alpha}$ in the following manner.
\begin{align*}
  \psi_{\alpha}(x) \coloneqq \exp(x^\alpha) - 1
\end{align*}
Using this function, we can define norms on random variables.
\begin{align*}
  \norm{X}_{\psi_{\alpha}} \coloneqq \inf \left( K > 0 \mid \E\left( \psi_{\alpha} \left( \frac{|X|}{K} \right) \right) \leq 1 \right)
\end{align*}
With this norm, the space of subexponential and subgaussian random variables form Banach spaces with respect to $\norm{\cdot}_{\psi_1}$ and $\norm{\cdot}_{\psi_2}$ respectively. These Banach spaces are often known as Orlicz spaces.

Our first application of Hoeffding-Chernoff-Azuma and Bernstein's inequality will be the Johnson-Lindenstrauss lemma\footnote{While this lemma was only a small, and rather easy, part of a hard technical paper of Johnson and Lindenstrauss, the lemma is (supposedly) one of the most cited lemmas in computer science.}.

\begin{theorem}[Johnson-Lindenstrauss lemma \cite{johnson1984extensions}]
  \label{thm:johnson-lindenstrauss}
  Let $F \subset \R^N$ be a finite set.
  Then for any $\varepsilon > 0$, there exists a linear mapping $\varphi: F \to \R^n$ with $n \leq \frac{C}{\varepsilon^2} \log(\#F)$ such that the mapping does not distort distances too much, i.e. the following inequalities hold for all $x$ and $y$ in $F$.
  \begin{align*}
    (1-\varepsilon) \norm{x-y}_2 \leq \norm{\varphi(x) - \varphi(y)}_2 \leq (1+\varepsilon) \norm{x-y}_2
  \end{align*}
\end{theorem}

  This is useful for computer scientists because it allow dimension reduction.
  Often, one has finitely many vectors in a very high dimensional space, and one only cares about their metric structure.
  This theorem allows one to reduce the ambient dimension significantly while not distorting the metric structure too much, and furthermore, the new ambient dimension is $O(\log \#F)$, whereas creating a metric graph would involve $O(\#F^2)$ computations.

While the $\ell^2$ norm is natural for geometry, in computer science applications, the $\ell^1$ norm is preferred, because of its relation to linear programming. So a natural follow up question is whether one can perform similar dimension reduction in $\ell^1$ instead.
This was open for a long time, but recently shown to be impossible (see \cite{10.1145/1089023.1089026}) in a very strong way.
It was shown that in order to have at most $\varepsilon$ distortion in the $\ell^1$ distance, the ambient dimension would be at least $C \# F$, i.e. linear in the size of the dataset, rather than logarithmic. The original proof is this fact was quite long and non-trivial, but Lee and Naor soon gave a simpler proof (see \cite{lee2004embedding}) that relied on some highly non-trivial functional analysis. Johnson and Naor also characterized Banach spaces that allow strong dimension reduction, and it turns out those spaces are quite similar to Hilbert spaces (see \cite{2008arXiv0807.1919J}).

\begin{proof}[Proof of Theorem \ref{thm:johnson-lindenstrauss}]
  Define a set $V \subset \R^n$ in the following manner.
  \begin{align*}
    V = \left\{ \frac{x-y}{\norm{x-y}_2} \mid \{x, y\} \subset F\text{ and }x \neq y \right\}
  \end{align*}
  We will work with this set $V$ instead.
  $V$ is contained in $S^N$, and has cardinality $\frac{\#F^2 - \#F}{2}$.

  Let $G$ be an $n \times N$ matrix with i.i.d subgaussian entries $g_{ij}$ satisfying the following two properties.
  \begin{align*}
    \E(g_{ij}) &= 0 \\
    \E(g_{ij}^2) &= 1
  \end{align*}
  Fix a point $v \in V$ and let $n = \frac{\theta}{\varepsilon^2} \log(\#F)$, where $\theta$ a parameter we'll pick later.
  We will prove that the following inequality holds with high probability.
  \begin{align*}
    \left| \norm{Gv}_2 - \sqrt{n} \right| \leq \varepsilon
  \end{align*}
  If the probability is high enough, we can do this for all elements of $V$ simultaneously using the union bound.

  Let $i \in \{1, \ldots, n\}$. Then we define $Y_i$ to be the $i$\textsuperscript{th} entry of $Gv$.
  \begin{align*}
    Y_i = \sum_{j=1}^N g_{ij}v_i
  \end{align*}
  We then compute the first and second moments of $Y_i$.
  \begin{align*}
    \E(Y_i) &= 0 \\
    \E(Y_i^2) &= \sum_{j=1}^{n} v_i^2 \E(g_{ij}^2) \\
            &= 1
  \end{align*}
  The random variable $Y_i$ is a linear combination of subgaussian random variable, and thus is also subgaussian. Also, as $i \neq i^{\prime}$, $Y_i$ and $Y_{i^{\prime}}$ are i.i.d.

  Let us now get estimates on $\norm{Gv}_2^2 - n$.
  \begin{align*}
    \norm{Gv}_2^2 - n &= \sum_{i=1}^n Y_i^2 - n \\
                      &= \sum_{i=1}^n (Y_i^2 - 1)
  \end{align*}
  Let $Z_i = Y_i^2 - 1$. Then $\E(Z_i) = 0$, and $Z_i$ are subexponential. This is a consequence of the fact that squares of subgaussian random variables are subexponential, which can be checked using the tail bound, or the moment condition.

  We use Bernstein's inequality to control the tail of the sum of $Z_i$.
  \begin{align*}
    \P\left( \left| \norm{Gv}_2^2 - n \right| > t \right)
    &= \P\left( \left| \sum_{i=1}^n Z_i \right| > t \right) \\
    &\leq 2\exp\left( -c \left( \frac{t^2}{n} \wedge t \right) \right)
  \end{align*}
  Set $t = \varepsilon n$, where $\varepsilon < 1$. In this regime, $t^2 < t$, which means the tail bound is $2 \exp\left( -c\frac{t^2}{n} \right)$.

  We now use the union bound to bound the probability that some $v \in V$ violates this condition.
  \begin{align*}
    \P\left( \exists\ v \in V \mid \left| \norm{Gv}_2^2 - n \right| > \varepsilon n \right)
    &\leq 2 \exp\left( -c \varepsilon^2 n \right) \cdot \#F \\
    &= 2 \exp\left( -c \varepsilon^2 \frac{\theta}{\varepsilon^2} \log(\#F) + \log(\#F) \right) \\
    &= \#F^{2(1 - c\theta)}
  \end{align*}
  A large enough $\theta$ makes the upper bound much smaller than $1$.

  Thus, with very high probability, $\left| \norm{Gv}_2^2 - n \right| \leq \varepsilon n$.
  We claim that this matrix $\varphi \coloneqq \frac{G}{\sqrt{n}}$ does the dimension reduction with distortion bounded by $\varepsilon$.
  This can be seen by expanding out the definition of $v$.
  \begin{align*}
    \varepsilon n
    &\geq \left| \norm{Gv}_2^2 - n \right| \\
    &= \left| \frac{\norm{Gx - Gy}_2^2}{\norm{x-y}_2^2} - n \right|
  \end{align*}
  Dividing both sides by $n$, we get the inequality we wanted.
  \begin{align*}
    \varepsilon \geq \left| \frac{\norm{\varphi x - \varphi y}_2^2}{\norm{x-y}_2^2} - 1 \right|
  \end{align*}
  This proves the result.
\end{proof}

\subsection{Concentration for quadratic forms}
\label{sec:conc-quadr-forms}

Let $\{X_i\}$ be i.i.d copes of a random variable, and let $A$ be a symmetric matrix associated to a quadratic form.
We define a new random variable $Y$ by evaluating the quadratic form $A$ on $\mathbf{X} = \left( X_1, \ldots, X_n \right)$.
\begin{align*}
  Y \coloneqq \langle \mathbf{X}, A\mathbf{X} \rangle
\end{align*}
Consider now the singular value decomposition of $A$, i.e. a decomposition as $UDV$, where $U$ and $V$ lie in $O(n)$ and $D$ is diagonal.
The diagonal entries of $D$ can be arranged to be non-decreasing, i.e. $s_1(A) \geq s_2(A) \geq \cdots \geq s_n(A) \geq 0$.
The diagonal entries are called the singular values of $A$.
The $j$\textsuperscript{th} singular value of $A$ is the square root of the $j$\textsuperscript{th} largest eigenvalue of $AA^{\top}$.

We also consider the Frobenius norm of the matrix $A$.
\begin{align*}
  \norm{A}_F \coloneqq \left( \sum_{i,j=1}^n a_{ij} \right)^{\frac{1}{2}}
\end{align*}
This is the inner-product norm for the following inner product on matrices.
\begin{align*}
  \langle A, B \rangle = \mathrm{tr}(AB^{\top})
\end{align*}
One can express the Frobenius norm of $A$ using the singular values.
\begin{align*}
  \norm{A}_F^2 = \sum_{i=1}^{n} s_i(A)^2
\end{align*}
Geometrically, the singular values are the lengths of the axes of the ellipsoid that is the image under $A$ of the standard sphere in $\R^n$.
Note that one can also express the operator norm in terms of the singular values: it's the $\sup$ norm on the singular values, just like the Frobenius norm is the $\ell^2$-norm on the singular values.

We can now state a concentration inequality for quadratic forms.
\begin{theorem}[Hanson-Wright inequality (\cite{hanson1971bound}  and \cite{wright1973bound})]
  \label{thm:hanson-wright}
  Let $A$ be any $n \times n$ matrix, and let $\{X_1, \ldots, X_n\}$ be i.i.d subgaussian random variables with $\E(X_i) = 0$.
  Let $\mathbf{X} = (X_1, \ldots, X_n) \in \R^n$. Then for any $t > 0$:
  \begin{align*}
    \P\left( \left| \langle \mathbf{X}, A \mathbf{X} \rangle - \E\left( \langle \mathbf{X}, A \mathbf{X} \rangle \right) \right| > t \right)
    \leq 2 \exp\left( -c \left( \frac{t^2}{\norm{A}_F^2} \wedge \frac{t}{\norm{A}_{\mathrm{op}}} \right) \right)
  \end{align*}
\end{theorem}
\begin{remark}
  Note that we can only expect a tail like we had in Bernstein's inequality, since entries of $\langle \mathbf{X}, A \mathbf{X} \rangle$ be squares of subgaussians, i.e. subexponential.
\end{remark}
\begin{remark}
  The original papers of Hanson and Wright proved a slightly weaker version of the above theorem.
  The version stated is from 2013, and appeared in \cite{rudelson2013}.
\end{remark}

\begin{proof}[Proof of Theorem \ref{thm:hanson-wright}]
  We first decompose $A$ as the sum of a diagonal matrix $A_{\mathrm{diag}}$, and a matrix $\wt{A}$ with all diagonal entries equal to $0$.
  \begin{align*}
    A = A_{\mathrm{diag}} + \wt{A}
  \end{align*}
  Since we are trying to bound the probability that $\langle \mathbf{X}, A \mathbf{X} \rangle$ deviates by more than $t$ from its mean, it will suffice to bound the probability that $\langle \mathbf{X}, A_{\mathrm{diag}}\mathbf{X} \rangle$ deviates by more than $\frac{t}{2}$ and the probability that $\langle \mathbf{X}, \wt{A} \mathbf{X} \rangle$ deviates by more than $\frac{t}{2}$.
  \begin{align*}
    \P\left( \left| \langle \mathbf{X}, A \mathbf{X} \rangle - \E\left( \langle \mathbf{X}, A \mathbf{X} \rangle \right) \right| > t \right)
    &\leq \P\left( \left| \langle \mathbf{X}, A_{\mathrm{diag}} \mathbf{X} \rangle - \E\left( \langle \mathbf{X}, A_{\mathrm{diag}} \mathbf{X} \rangle \right) \right| > \frac{t}{2} \right) \\
      &+ \P\left( \left| \langle \mathbf{X}, \wt{A} \mathbf{X} \rangle - \E\left( \langle \mathbf{X}, \wt{A} \mathbf{X} \rangle \right) \right| > \frac{t}{2} \right)
  \end{align*}
  Note that since $\wt{A}$ has zeroes on the diagonal, and the $X_i$s are independent with mean $0$, that means $\E\left( \langle \mathbf{X}, \wt{A} \mathbf{X} \rangle \right) = 0$.

  Observe that the first term is a tail bound on a sum of subexponential random variables.
  \begin{align*}
    \langle \mathbf{X}, A_{\mathrm{diag}} \mathbf{X} \rangle - \E\left( \langle \mathbf{X}, A_{\mathrm{diag}} \mathbf{X} \rangle \right)
    &= \sum_{i=1}^{n} a_{ii}\left( X_i^2 - \E(X_i^2) \right)
  \end{align*}
  Since $X_i$ are subgaussian random variables, $X_i^2 - \E(X_i^2)$ are subexponential random variables with mean $0$, which means we can apply Bernstein's inequality.
  \begin{align*}
    \P\left( \left| \langle \mathbf{X}, A_{\mathrm{diag}} \mathbf{X} \rangle - \E\left( \langle \mathbf{X}, A_{\mathrm{diag}} \mathbf{X} \rangle \right) \right| > \frac{t}{2} \right)
    \leq 2 \exp\left( -c \left( \frac{t^2}{\sum_{i=1}^n a_{ii}^2} \wedge \frac{t}{\max(a_{ii})} \right) \right)
  \end{align*}
  Note that we can bound the denominators using the appropriate matrix norm.
  \begin{align*}
    \sum_{i=1}^{n} a_{ii}^2 &\leq \norm{A}_F^2 \\
    \max(a_{ii}) &\leq \norm{A}
  \end{align*}
  Note that the upper bound for the diagonal term is of the form stated in the theorem, which means that all we need to do prove a similar or better bound for the second term, which involves $\wt{A}$.
  That boils down to finding a probability bound for the following event.
  For convenience of notation, we will now denote $\wt{A}$ by just $A$, where it is understood that $A$ is a matrix with all diagonal entries equal to $0$.
  We will also denote $\langle \mathbf{X}, A \mathbf{X} \rangle$ by $Y$.
  \begin{align*}
    \P\left( \left| Y \right| > t \right)
    \leq 2 \exp\left( -c \left( \frac{t^2}{\norm{A}_F^2} \wedge \frac{t}{\norm{A}_{\mathrm{op}}} \right) \right)
  \end{align*}
  Recall that when we proved Bernstein's inequality, we did so bounding the Laplace transform of $Y$, i.e. getting upper bounds for $\E\left( \exp\left( \lambda Y \right) \right)$.
  Since $Y$ in our case is a quadratic function in independent random variables $X_i$, estimating the Laplace transform is a little tricky, and we need to use \emph{decoupling}.

  Introduce new random variables $\{\delta_1, \ldots, \delta_n\}$ which are independent from each other, as well as all $X_i$, and are distributed like $\mathrm{Bernoulli}\left( \frac{1}{2} \right)$. We then have the following.
  \begin{align*}
    \E\left( \exp(\lambda Y) \right)
    &= \E\left(  \exp\left( 4\lambda \sum_{i,j = 1}^{n} \E_{\delta}\left( \delta_i(1- \delta_j) \right) a_{ij} X_i X_{j} \right) \right)
  \end{align*}
  Here, $\E_{\delta}$ represents taking expectation over the sample space of the $\delta_{i}$, and $\E$ represents taking the expectation over the sample space of $X_i$.
  Note that the equality holds because for $i=j$, $\delta_i(1-\delta_j) = 0$.
  We now use Jensen's inequality to pull out the $\E_{\delta}$.
  \begin{align*}
    \E\left( \exp(\lambda Y) \right)
    &\leq \E \E_{\delta}\left(  \exp\left( 4\lambda \sum_{i,j = 1}^{n} \left( \delta_i(1- \delta_j) \right) a_{ij} X_i X_{j} \right) \right)
  \end{align*}
  Let $I$ be the random subset of $\{1, \ldots, n\}$ where $i \in I$ iff $\delta_i = 1$.
  We can condition the above expectation on the value of $I$ to simplify things.
  \begin{align*}
    \E\left( \exp(\lambda Y) \right)
    &\leq \E_{\delta} \E \left( \exp\left( 4\lambda \sum_{i \in I} \sum_{j \not \in I} a_{ij} X_i X_j \right) \bigg|\ I \right)
  \end{align*}
  We can simplify the inner integral using the fact that $I$ is fixed.
  \begin{align*}
    \E \left( \exp\left( 4\lambda \sum_{i \in I} \sum_{j \not \in I} a_{ij} X_i X_j \right) \bigg|\ I \right)
    &= \prod_{j \not \in I} \E \exp\left( \left[ 4 \lambda \sum_{i \in I} a_{ij} X_i \right] \cdot X_j \right)
  \end{align*}
  We now use the fact that each $X_j$ is subgaussian with mean $0$ to bound the right hand side in the following manner.
  \begin{align}
    \label{eq:hw-1}
    \prod_{j \not \in I} \E \exp\left( \left[ 4 \lambda \sum_{i \in I} a_{ij} X_i \right] \cdot X_j \right)
    &\leq \prod_{j \not \in I} \E \left( \exp\left[ 16 C \lambda^2 \left( \sum_{i \in I} a_{ij} X_i \right)^2 \right] \right)
  \end{align}
  We now use a trick to turn the expectation of the quadratic form as in \eqref{eq:hw-1} to expectation of a bilinear form.
  Note that for a standard normal random variable $g$, one has the following explicit formula for the Laplace transform.
  \begin{align*}
    \E\left( \theta g \right) = \exp\left( \frac{\theta^2}{2} \right)
  \end{align*}
  Let $\{g_1, \ldots, g_n\}$ be i.i.d standard normal random variables that are independent of $X_{i}$ and $\delta_i$.
  We get that the right hand side of \eqref{eq:hw-1} is the following.
  \begin{align*}
    \prod_{j \not \in I} \E \left( \exp\left[ 16 C \lambda^2 \left( \sum_{i \in I} a_{ij} X_i \right)^2 \right] \right)
    = \E \left( \exp\left( C^{\prime}\lambda \sum_{j \not \in I} \sum_{i \in I} a_{ij} X_i g_j \right) \right)
  \end{align*}
  Repeating this whole process again, with $X_i$ rather than $X_j$, we end up with the following upper bound.
  \begin{align*}
    \E \left( \exp\left( 4\lambda \sum_{i \in I} \sum_{j \not \in I} a_{ij} X_i X_j \right) \bigg|\ I \right)
    &\leq \E\left( \exp\left( C^{\prime \prime} \lambda^2 \sum_{i \in I} \left( \sum_{j \not \in I} a_{ij} g_j \right)^2 \right) \bigg|\ I \right)
  \end{align*}
  Note that the upper bound is independent of $X_i$, and only depends on the Bernoulli random variables $\delta_i$ and the normal random variables $g_i$.

  We will now use that fact that $\mathbf{g} = (g_1, \ldots, g_n)$ is a rotation invariant random vector in $\R^n$ to estimate the upper bound.
  Let $P_{I}$ be the projection to the subspace spanned by $\{e_i\}_{i \in I}$, and let $B_I = P_{I}A(\mathrm{Id} - P_I)$.
  Then the innermost double sum can be expressed as a norm.
  \begin{align*}
    \sum_{i \in I} \left( \sum_{j \not \in I} a_{ij} g_j \right)^2 = \norm{B_I \mathbf{g}}_2^2
  \end{align*}
  This means we need to estimate the following conditional expectation.
  \begin{align*}
    \E\left( C^{\prime \prime} \lambda^2 \norm{B_I \mathbf{g}}_2^2 \bigg|\ I \right)
  \end{align*}
  Let $U_ID_IV_I$ be the singular value decomposition of $B_I$.
  Since $\norm{\cdot}_2$ is invariant under $O(n)$ action, and the distribution of $\mathbf{g}$ is also $O(n)$ invariant, $B_I \mathbf{g}$ has the same distribution as $D_I \mathbf{g}$.
  This means we really just need to understand the distribution of the singular values of $B_I$.
  \begin{align*}
    \E\left( \exp\left(   C^{\prime \prime} \lambda^2 \norm{B_I \mathbf{g}}_2^2 \right) \bigg|\ I \right)
    &= \E\left( \exp\left( C^{\prime \prime} \lambda^2 \norm{D_I \mathbf{g}}_2^2 \right)  \bigg|\ I \right) \\
    &= \E\left( \exp\left(  C^{\prime \prime} \lambda^2 \sum_{j=1}^n s_j^2(B_I) g_j^2 \right)  \bigg|\ I \right) \\
    &= \prod_{j=1}^n \E\left( \exp\left( C^{\prime \prime} \lambda^2 s_j^2(B_I) g_j^2 \right) \bigg|\ I \right)
  \end{align*}
  One can explicitly compute $\E(\exp(\theta g^2))$ for a normal random variable $g$ for $\theta \in \left[ 0, \frac{1}{2} \right]$.
  \begin{align*}
    \E(\exp\left( \theta g^2 \right)) = \frac{1}{\sqrt{1 - 2\theta}}
  \end{align*}
  Hence,
  \begin{align*}
    \E\left( C^{\prime \prime} \lambda^2 \norm{B_I \mathbf{g}}_2^2 \bigg|\ I \right)
    &= \prod_{j=1}^n \left( 1 - 2 C^{\prime \prime} \lambda^2 s_j^2(B_I) \right)^{-\frac{1}{2}}
  \end{align*}
  For $x \in \left[ 0 , \frac{1}{2} \right]$, $\frac{1}{\sqrt{1-x}} \leq e^x$. This lets us bound the right hand side in the following manner.
  \begin{align*}
    \prod_{j=1}^n \left( 1 - 2 C^{\prime \prime} \lambda^2 s_j^2(B_I) \right)^{-\frac{1}{2}}
    &\leq \prod_{j=1}^n \exp\left( 2 C^{\prime \prime} \lambda^2 s_j^2(B_I) \right)
  \end{align*}
  Note however that to use the simplification, we required $2 C^{\prime \prime} \lambda^2 s_j^2(B_I) \leq \frac{1}{2}$.
  If we pick a $\lambda$ smaller than $\frac{c^{\prime \prime \prime}}{s_1(B_I)}$, then the inequalities are valid.
  That ends up giving the following bound.
  \begin{align*}
    \E\left( \exp\left( C \lambda \norm{B_I \mathbf{g}}_2^2 \right) \right)
    &\leq \exp\left( \wt{C} \lambda^2 \norm{B_I}_F^2 \right)
  \end{align*}
  This is valid when $\lambda \leq \frac{c^{\prime \prime \prime}}{s_1(B_I)} = \frac{c^{\prime \prime \prime}}{\norm{B_I}}$.
  Finally, we can get rid of the dependence on $I$ using the following inequalities involving matrix norms.
  \begin{align*}
    \norm{B_I}_F &= \norm{P_IA(\mathrm{Id} - P_I)}_F \\ &\leq \norm{A}_F \\
    \norm{B_I} &= \norm{P_IA(\mathrm{Id} - P_I)} \\ &\leq \norm{A}
  \end{align*}
  We can now conclude our estimates. We get the following inequality for all $\lambda$ less than $\frac{c^{\prime \prime \prime}}{\norm{A}}$.
  \begin{align*}
    \E\left( \exp(\lambda Y) \right)
    &\leq \E_{\delta} \E\left( \exp\left( C \lambda \norm{B_I \mathbf{g}}_2^2 \right) \right) \\
    &\leq \exp\left( \wt{C} \lambda^2 \norm{B_I}_F^2 \right) \\
    &\leq \exp\left( \wt{C} \lambda^2 \norm{A}_F^2 \right)
  \end{align*}
  We are able to integrate easily over the $\delta$ sample space because the right hand side does not depend on $I$ at all.
  The rest of the proof follows exactly like the end of Bernstein's inequality, where we optimized over $\lambda$ to get the best bound via Markov's inequality.
\end{proof}

We now show a rather unexpected application of the Hanson-Wright inequality.
\begin{theorem}
 Let $A$ be an $n \times n$ matrix, and let $\mathbf{X} \in \R^n$ be a random vector with i.i.d subgaussian coordinates with mean $0$ and variance $1$.
 Then for any $\tau > 0$, the following holds.
 \begin{align*}
   \P\left( \left| \norm{A \mathbf{X}}_2 - \norm{A}_F \right| > \tau \right)
   \leq 2 \exp\left( -c \frac{\tau^2}{\norm{A}^2} \right)
 \end{align*}
\end{theorem}
\begin{remark}
  This result is surprising for two reasons: first, we will prove this using the Hanson-Wright inequality, but the tail bound in this inequality is better than the tail bound in Hanson-Wright. Second, all of our earlier concentration inequalities were concentration inequalities about the mean, but in this case $\E\left( \norm{A \mathbf{X}}_2 \right) \neq \norm{A}_F$.
\end{remark}

\begin{proof}
  Let $Y = \norm{A \mathbf{X}}_2^2 = \langle \mathbf{X}, A^{\top}A \mathbf{X} \rangle$.
  This expression as the quadratic form $A^\top A$ will let us apply the Hanson-Wright inequality. Using the Hanson-Wright inequality, we obtain the following.
  \begin{align*}
    \P\left( \left| \langle \mathbf{X}, A^{\top}A \mathbf{X} \rangle
    - \E\left( \langle \mathbf{X}, A^{\top}A \mathbf{X} \rangle \right)
    \right| > t \right)
    \leq 2 \exp\left( -c \left[ \frac{t^2}{\norm{A^{\top}A}_F^2} \wedge \frac{t}{\norm{A^{\top}A}} \right] \right)
  \end{align*}
  We can explicitly evaluate $\E \left( \langle \mathbf{X}, A^{\top}A \mathbf{X} \rangle \right)$: since the entries of $\mathbf{X}$ are i.i.d with mean $0$ and variance $1$, the expectation simplifies to $\mathrm{tr}(A^{\top}A) = \norm{A}_F^2$.
  We also have these inequalities involving products of matrices.
  \begin{align*}
    \norm{AB}_F &\leq \norm{A} \cdot \norm{B}_F \\
    \norm{AB} &\leq \norm{A} \cdot \norm{B}
  \end{align*}
  Using these two inequalities, we can simplify the inequality we got from the application of Hanson-Wright inequality.
  \begin{align*}
    \P\left( \left| \norm{A \mathbf{X}}_2^2 - \norm{A}_F^2 \right| > t \right)
    \leq 2 \exp\left( -c \left[ \frac{t^2}{\norm{A}^2 \cdot \norm{A}_F^2} \wedge \frac{t}{\norm{A}^2} \right] \right)
  \end{align*}
  Let $t = \varepsilon \cdot \norm{A}_F^2$. Then the above inequality can be rewritten as follows.
  \begin{align*}
    \P\left( \left| \frac{ \norm{A \mathbf{X}}_2^2}{ \norm{A}_F^2}  - 1 \right| > \varepsilon \right)
    \leq 2 \exp\left( -c \frac{\norm{A}_F^2}{\norm{A}^2} \left[ \varepsilon^2 \wedge \varepsilon \right] \right)
  \end{align*}
  Rewriting the inequality in this manner makes it clearer which of the terms in the upper bound is smaller.
  The analysis splits into two cases.
  \begin{description}
  \item[\textbf{Case 1} ($\varepsilon \leq 1$):] In this case, $\varepsilon^2 \leq \varepsilon$, so the tail bound simplifies in the following manner.
    \begin{align*}
    \P\left( \left| \frac{ \norm{A \mathbf{X}}_2^2}{ \norm{A}_F^2}  - 1 \right| > \varepsilon \right)
      \leq 2 \exp \left( -c \frac{\norm{A}_F^2}{\norm{A}^2} \varepsilon^2 \right)
    \end{align*}
    Observe now that for positive values of $x$, $|x^2-1| \geq |x-1|$. This means that we can replace the $\frac{\norm{A \mathbf{X}}_2^2}{\norm{A}_F^2}$ in left hand side of the above inequality by its square root.
    We do that, and set $\tau = \varepsilon \norm{A}_F$.
    \begin{align*}
      \P\left( |\norm{A \mathbf{X}}_2 - \norm{A}_F| > \tau \right)
      \leq 2 \exp \left( -c \frac{\tau^2}{\norm{A}^2} \right)
    \end{align*}
    This proves the inequality for all $\tau \in \left[ 0, \norm{A}_F \right]$.
  \item[\textbf{Case 2} ($\varepsilon > 1$):] In this case $\varepsilon \leq \varepsilon^2$, so the tail bound simplifies in the following manner.
    \begin{align*}
    \P\left( \left| \frac{ \norm{A \mathbf{X}}_2^2}{ \norm{A}_F^2}  - 1 \right| > \varepsilon \right)
      \leq 2 \exp \left( -c \frac{\norm{A}_F^2}{\norm{A}^2} \varepsilon \right)
    \end{align*}
    Note again that for positive values of $x$, $|x^2 - 1| \geq |x-1|^2$. We use this fact to make a substitution on the left hand side, and let $\tau = \sqrt{\varepsilon} \norm{A}_F$.
    \begin{align*}
      \P\left( |\norm{A \mathbf{X}}_2 - \norm{A}_F| > \tau \right)
      \leq 2 \exp  \left( -c \frac{\norm{A}_F^2}{\norm{A}^2} \varepsilon \right)
    \end{align*}
    This proves in the inequality for $\tau \in \left( \norm{A}_F, \infty \right)$.
  \end{description}
\end{proof}

\printbibliography

\end{document}
