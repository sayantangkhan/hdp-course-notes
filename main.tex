\documentclass[11pt]{article}

%%% The preamble loads packages, theorem styles, and macros %%%%%%%%%%%%%%
\input{CourseNotesPreamble}

\title{Math 626: High Dimensional Probability}
\author{Taught by Mark Rudelson \\ Lecture notes by Sayantan Khan}
%\email{schommerpries.chris.math@gmail.com}
%\address{Department of Mathematics \\
%Harvard University \\
%1 Oxford St. \\
%Cambridge, MA 02138}
\date{\today}

\begin{document}


\maketitle
\tableofcontents

\section{Introduction}
\label{sec:introduction}

High dimensional probability is the study of random variables taking values in $\RR^n$ for large but fixed values of $n$.
While this area has always been studied by classical probability theorists, it has also attracted attention from computer scientists, especially since the design and analysis of fast probabilistic algorithms requires tools and theorems from this field.
A classical result that arises from pure mathematics, but has several real life applications is Dvoretzky's theorem, whose statement does not involve any probability at all, and yet the proof uses concentration inequalities.
\begin{theorem}[Dvoretzky's theorem]
  Let $X$ be an $n$-dimensional normed vector space. Then for any $\varepsilon > 0$, there exists a constant $c(\varepsilon) > 0$, and a linear subspace $E$ of $X$ such that, $\dim(E) \geq c(\varepsilon) \log(n)$, and for all $v \in E$, the following inequality relating the ambient norm and the Euclidean norm on $E$.
  \begin{align*}
    (1-\varepsilon) \norm{v}_2 \leq \frac{\norm{v}_X}{M(X)} \leq (1+\varepsilon)\norm{x}_2
  \end{align*}
  Where $M(X)$ is a scaling factor that depends only on $X$, and $\norm{\cdot}_X$ and $\norm{\cdot}_2$ are the ambient and Euclidean norm on $E$.
\end{theorem}

\begin{proof}[Idea of proof]
  Pick a random subspace, and then show that with very high probability, the given inequality holds.
  We will prove the result in full detail later in the course.
\end{proof}

\begin{remark}
  \label{rem1}
  When the norm on $X$ is the $\ell^1$ norm, the lower bound on the dimension of $E$ can be improved to be linear in $c(\varepsilon) n$.
\end{remark}

\paragraph{A computer science application of Dvoretzky's theorem}

Consider a subset $S$ of $\left(\ell^2\right)^N$, given by inequalities involving the norms of elements in $\left( \ell^2 \right)N$.
Suppose that we are required to optimize a linear function $f$ on the set $S$.
Since $S$ is given by inequalities involve the $\ell^2$-norm, it will be and intersection of interiors of ellipsoids, and consequently, optimizing $f$ will be computationally expensive.
But we can get around the computational expense by embedding $\left(\ell^2\right)^N$ into $\ell_1^M$, where $M$ is $O(N)$, by Remark \ref{rem1}.
Since this embedding does not distort distances too much, we can replace $S$ with a nearby polytope, given by inequalities involving the $\ell^1$ norm instead.
Optimizing a linear function on a polytope is computationally much easier, thanks to linear programming.

\paragraph{Empirical covariance estimation}

Let $X$ be an $\RR^n$-valued random variable, and let $\E(X) = 0$.
The covariance of $X$ is $\E(X^{\top}X)$, and will be denoted by $A$.
Let $\left\{ X_1, X_2, \ldots, X_m \right\}$ be i.i.d. samples of $X$.
We define the sample covariance $A_m$ in the following manner.
\begin{align*}
  A_n = \frac{\sum_{i=1}^m X_i^{\top}X_i}{m}
\end{align*}
As $m$ tends to infinity, the sample covariance $A_m$ will approach the true covariance, as we would expect the law of large numbers to predict.
A harder, and more interesting question is to determine how many samples do we need to take to be within some threshold of the true covariance with high probability.

Just like in the scalar setting, one answers the question by proving appropriate concentration inequalities for matrix valued random variables.
Here is the most general set up: Let $A_m$ and $A$ be matrices mapping some normed space $X$ to some other normed space $Y$.
We define the distance between $A_m$ and $A$ using the operator norm.
\begin{align*}
  \norm{A_m - A}_{\mathrm{op}} &= \max_{\norm{v}_x \leq 1} \norm{(A_m-A)v}_Y \\
                               &= \max_{\norm{v}_x \leq 1} \max_{\norm{w}_{Y^{\ast}} \leq 1} w\left( (A_m - A)v \right) \\
                               &= \max_{\substack{(v, w) \in X \times Y^{\ast} \\ \norm{v} \leq 1 \\ \norm{w} \leq 1}} w((A_m - A)v)
\end{align*}
We can consider $w((A_m -A)v)$ to be a family of scalar random variables parameterized by points in $X \times Y^{\ast}$, i.e. a random process $V_u$ parameterized by $u \in X \times Y^{\ast}$.
This leads to the following two questions.
\begin{enumerate}[(i)]
\item How to bound $\E \max V_u$.
\item How to bound $\P(\left| \max V_u - \E \max V_u \right| \geq t)$.
\end{enumerate}
It turns out one can often answer (ii) without answering (i), which may seem surprising given that the most elementary concentration inequalities involve moment bounds (i.e. Markov's inequality).
The starting point in answering (ii) is understanding \emph{concentration of measure}.

\section{Concentration of measure}
\label{sec:conc-meas}

Concentration of measure was originally observed LÃ©vy, but first used by Milman in the early 70s.
Roughly speaking, concentration of measure is the following phenomenon: suppose $(X, d, \P)$ is a metric space endowed with a probability measure and $f: X \to \RR$ is a ``nice'' function.
Then the value of $f$ is essentially constant, i.e. there exists some constant $M(f)$ such that for small enough $\varepsilon$, the following probability bound holds.
\begin{align*}
  \P(|f(x) - M(f)| < \varepsilon) \ll 1
\end{align*}
Usually by nice, we will mean $1$-Lipschitz, although similar results hold for a more general class of functions like convex or quasi-convex functions.
We begin with the simplest version of measure concentration.

\paragraph{Concentration for linear functions}

Let the metric space $X$ in this setting be $\R^n$ for some fixed $n$, and let $\{X_1, \ldots, X_n \}$ be i.i.d scalar random variables.
Let $f: \R^n \to \R$ be a linear function, given by taking inner product with some vector $\mathbf{a}$. We define $Y$ to be $\sum_{i=1}^n a_i X_i$.
We start by considering random variables $X_i$ which have a subgaussian decay.

\begin{definition}[Subgaussian decay]
  A random variable $X$ is said to be $\sigma$-subgaussian if there exists a constant $C > 0$, such that for all $t > 0$, the following inequality holds.
  \begin{align*}
    \P(|X| > t) \leq C \exp\left( -\frac{1}{2} \left( \frac{t}{\sigma} \right)^2 \right)
  \end{align*}
\end{definition}
\begin{remark}
  The constant $\sigma$ is often referred to as the variance proxy of the subgaussian random variable.
\end{remark}

\begin{example}
  The following random variables are examples of subgaussian random variables.
  \begin{enumerate}[(i)]
  \item Normal random variables
  \item Bounded random variables
  \end{enumerate}
\end{example}

\begin{lemma}
  Let $X$ be a random variable. Then the following are equivalent:
  \begin{enumerate}[(i)]
  \item For all $t> 0$, $\P(|X| > t) \leq C \exp\left( -\frac{1}{2} \left( \frac{t}{\sigma} \right)^2 \right)$ (definition of subgaussian random variables).
  \item There exists $a > 0$ such that $\E(\exp(aX^2)) < \infty$ ($\psi_2$ condition).
  \item There exist $C^{\prime}$ and $b$ such that for all $\lambda \in \R$, $\E(\exp(\lambda X)) \leq C^{\prime} \exp(b \lambda^2)$ (Laplace transform condition).
  \item There exists $K$ such that for all $p \geq 1$, $\E(X^p)^{\frac{1}{p}} \leq K \sqrt{p}$ (moment condition).
  \end{enumerate}
  Moreover, if $\E(X) = 0$, then the constant $C^{\prime}$ in the Laplace transform condition can be taken to be equal to $1$.
\end{lemma}

\begin{proof}
  \begin{description}
  \item[$(i) \implies (ii)$] Using Fubini's theorem and a change of variables, we can express $\E(aX^2)$ as an integral involving tail bounds.
    \begin{align*}
      \E(aX^2) &= 1 + \int_0^{\infty} 2 a t e^{at^2} \cdot \P\left( |X| > t \right) \dd t \\
      &\leq 1 + \int_0^\infty 2Cat\exp\left( at^2 - \frac{1}{2}\left( \frac{t}{\sigma}  \right)^2 \right) \dd t
    \end{align*}
    Clearly, picking a value of $a$ smaller than $\frac{1}{2\sigma^2}$ will make the integral converge.
  \item[$(ii) \implies (iii)$] Since we want to estimate the expectation of $\exp(\lambda X)$, we multiply and divide by $\exp(aX^2)$ and complete the square.
    \begin{align*}
      \exp(\lambda X) = \exp\left(aX^2\right) \cdot \exp\left( \frac{\lambda^2}{4a} \right) \cdot \exp\left( - \left( \sqrt{a}X + \frac{\lambda}{2\sqrt{a}} \right)^2 \right)
    \end{align*}
    Note that the third term in the product is always less than $1$. We now take the expectation of the right hand side.
    \begin{align*}
      \E(\exp(\lambda X)) &= \E\left( \exp\left(aX^2\right) \cdot \exp\left( \frac{\lambda^2}{4a} \right) \cdot \exp\left( - \left( \sqrt{a}X + \frac{\lambda}{2\sqrt{a}} \right)^2 \right) \right) \\
      &\leq \exp\left( \frac{\lambda^2}{4a} \right) \E\left( \exp(aX^2) \right)
    \end{align*}
    Setting $b= \frac{1}{4a}$ and $C^{\prime} = \E(\exp(aX^2))$, we get the result.
  \item[$(iii) \implies (iv)$] We begin by getting a crude estimate for $\E(X^p)$ using the infinite series for $\exp(\lambda X)$.
    \begin{align*}
      \E(X^p) &\leq \frac{p!}{\lambda^p} \E(\exp(\lambda X)) \\
              &= \frac{C^{\prime}p! \exp(b\lambda^2)}{\lambda^p}
    \end{align*}
    Note that this inequality works for all values of $\lambda$, but to get the best inequality, we minimize the right hand side by varying $\lambda$ over $\R$.
    The minimum is attained when $\lambda = \frac{\sqrt{p}}{2b}$: plugging that into the right hand side, and taking $p$\textsuperscript{th} roots gives us the following.
    \begin{align*}
      \E(X^p)^{\frac{1}{p}} \leq C^{\prime \prime} \frac{(p!)^{\frac{1}{p}}}{\sqrt{p}}
    \end{align*}
    Here, we have absorbed all the constants into $C^{\prime \prime}$.
    Using Stirling's approximation, the numerator is bounded above by $p$, giving us the inequality we want.
  \item[$(iv) \implies (i)$] We rewrite the event $|X| > t$ in the following manner.
    \begin{align*}
      \P(|X| > t) &= \P(\exp(\lambda X^2) > \exp(\lambda t^2)) \\
                  &\leq \exp(-\lambda t^2) \E(\exp(\lambda X^2))
    \end{align*}
    Here, $\lambda$ is a positive real number that we will specify later, and the inequality comes from Markov's inequality.
    Of course, we do not a priori know that $\E(\lambda X^2)$ is finite, but we will pick a $\lambda$ small enough such that it is.
    Using Fubini's theorem, we can express $\E(\exp(\lambda X^2))$ in the following manner.
    \begin{align*}
      \E(\exp(\lambda X^2)) = 1 + \frac{\lambda \E(X^2)}{1!} + \frac{\lambda^2 \E(X^4)}{2!} + \cdots
    \end{align*}
    Using the bound on the moments of $X$ and Stirling's approximation, we get the following inequality.
    \begin{align*}
      \E(\exp(\lambda X^2)) &\leq \sum_{p=0}^{\infty} \frac{(2\lambda K^2p)^p}{p!} \\
                            &\leq \sum_{p=0}^{\infty} \frac{(2\lambda e K^2 p)^p}{\sqrt{2\pi p} p ^p}
    \end{align*}
    If we pick $\lambda$ to be small enough that $2e \lambda K^2$ is much smaller than $1$, then the infinite sum converges, and the expectation is finite.
    Setting $\frac{1}{2\sigma^2}$ to be equal to $\lambda$ gives us the result.
  \end{description}
  We now show that the constant in the Laplace transform condition can be set to be $1$ when $\E(X) = 0$.
  To do so, we recall the $\psi_2$ and the Laplace transform condition, i.e. there exist constants $a$, $C^{\prime}$ and $b$ such that the following two inequalities hold for all $\lambda \in \R$.
  \begin{align}
    \label{eq:1}
    \E(\exp(aX^2)) &< \infty \\
    \E(\exp(\lambda X)) & \leq C \exp(b\lambda^2)
  \end{align}
  Suppose now that $\lambda^2 > 2a$.
  By the Laplace transform condition, we have the following inequality.
  \begin{align*}
    \E(\exp(2aX)) &\leq C^{\prime} \exp(4ba^2) \\
                  &= \exp(4a^2 b^{\prime})
  \end{align*}
  Where $b^{\prime}$ is $b + \frac{\log(C^{\prime})}{4a^2}$.
  Since $b^{\prime}$ decreases as $a$ increases, for any $\lambda^2 > 2a$, $\E(\exp(aX^2))$ will be less than $\exp(b^{\prime} \lambda^2)$.

  Now suppose that $\lambda^2 < 2a$.
  We begin by considering the special case where $X$ is a symmetric random variable.
  By symmetry of $X$, we have the following inequality.
  \begin{align*}
    \exp(\lambda X) &= \frac{\exp(\lambda X) + \exp(-\lambda X)}{2} \\
            &\leq \exp\left( \frac{\lambda^2 X^2}{2} \right)
  \end{align*}
  Taking expectations on both sides gives us the following.
  \begin{align*}
    \E(\exp(\lambda X)) \leq \E\left(\exp\left( \frac{\lambda^2}{2} X^2 \right)\right)
  \end{align*}
  Since $\lambda^2 < 2a$, $\frac{2a}{\lambda^2}$ is greater than $1$, and we can use HÃ¶lder's inequality to bound the right hand term.
  \begin{align*}
    \E\left(\exp\left( \frac{\lambda^2}{2} X^2 \right)\right) \leq
    \left( \E\left(\exp\left(   aX^2   \right) \right) \right)^{\frac{\lambda^2}{2a}}
  \end{align*}
  Since $\E(\exp(aX^2))$ is a finite constant, the right hand side is $\exp(b^{\prime \prime} \lambda^2)$ for some constant $b^{\prime \prime}$.

  Now suppose $X$ is not symmetric.
  Let $X^{\prime}$ be an identical independent copy of $X$.
  Since $\E(X^{\prime})$ is $0$, we have the following equality.
  \begin{align*}
    \E(\exp(\lambda X)) = \E(\exp(\lambda (X - \E(X^{\prime}))))
  \end{align*}
  Since $\exp$ is a convex function, we can pull out the inner expectation, using Jensen's inequality.
  \begin{align*}
    \E(\exp(\lambda X)) \leq \exp(\lambda(X - X^{\prime}))
  \end{align*}
  Since $X - X^{\prime}$ is symmetric, the result follows from the previous part, and the proof is complete.
\end{proof}
We now explain why care so much about the constant in the Laplace transform condition.

\begin{theorem}[Hoeffding-Chernoff-Azuma inequality]
  \label{thm:hoeffding}
  Let $\{X_1, \ldots, X_n\}$ be i.i.d. subgaussian random variables with mean $0$.
  Then for any $(a_1, \ldots , a_n) \in \R^n$ and any $t > 0$, the following probability bound holds.
  \begin{align*}
    \P\left( \left| \sum_{i=1}^n a_i X_i \right| > t \right) \leq C \exp\left( -c \frac{t^2}{\norm{\mathbf{a}}_2} \right)
  \end{align*}
  Where $C$ and $c$ are some absolute constants.
\end{theorem}
\begin{proof}
  Without loss of generality, we can assume $\norm{\mathbf{a}}_2 = 1$.
  It will suffice to show that the sum of subgaussian random variables is subgaussian.
  We will show it verifying the Laplace transform condition.
  Let $\lambda \in \R$, and let $Y = \sum_{i=1}^n a_iX_i$.
  We compute $\E(\exp(\lambda Y))$.
  \begin{align*}
    \E(\exp(\lambda Y)) &= \prod_{i=1}^n \E(\exp(\lambda a_i X_i)) \\
                        &\leq \prod_{i=1}^n \E(\exp(b \lambda^2 a_i^2)) \\
                        &= \E(\exp(b\lambda^2))
  \end{align*}
  This proves the result. Note that having the Laplace transform coefficient equal to $1$ helped, because if the coefficient $C$ was greater than $1$, then we would pick up a constant of $C^n$, which would be very large for large values of $n$.
\end{proof}

To see how this inequality is used in practice, consider the simplest possible example of a subgaussian random variable, a random variable that takes values $1$ and $-1$ with probability $\frac{1}{2}$.
% A Rademacher random variable is a random variable that takes values $1$ and $-1$ with probability $\frac{1}{2}$.

Let's recall some elementary facts about Fourier analysis before stating the example.
Let $L^2([0,1])$ be the space of complex $L^2$ functions on $[0,1]$ and let $\{e_n\}_{n \in \Z}$ be the standard Fourier basis, i.e. $e_n(t) = \exp(2\pi i nt)$.
Since $\{e_n\}$ forms an orthonormal basis, any function $f \in L^2([0,1])$ can be decomposed into its Fourier series.
\begin{align*}
  f = \sum_{n \in \Z} \widehat{f}(n) e_n
\end{align*}
The Fourier coefficient $\widehat{f}(n)$ is given by $\int_0^1 f(t) \overline{e_n(t)} \dd t$.
The map sending $f$ to its Fourier coefficients is a linear isometry from $L^2([0,1])$ to $\ell^2(\Z)$.
For most sequences $\{\widehat{f}(n)\}$ in $\ell^2(\Z)$, the associated function $f$ will not be continuous, but we will show that under reasonably mild conditions on $\{\widehat{f}(n)\}$, $f$ can be made to be continuous.

Let $\varepsilon_n \in \{-1, 1\}$ for $n \in \Z$, and let $\varepsilon = \{\varepsilon_n\}_{n \in \Z}$.
For any $f \in L^2([0,1])$ and any such $\\varepsilon$, define $f_{\varepsilon}$ to be the following function.
\begin{align*}
  f_{\varepsilon} = \sum_{n \in Z} \varepsilon_n \widehat{f}(n) e_n
\end{align*}
We then have the following theorem.
\begin{theorem}
  \label{thm:continuous-l2-function}
  Let $f$ be a function in $L^2([0,1])$ whose Fourier coefficients satisfy the following inequality.
  \begin{align}
    \label{eq:convergence-condition-example}
    \sum_{n \in \Z} \left( \log(|n| + 1) \right)^3 \left| \widehat{f}(n) \right|^2 < \infty
  \end{align}
  Let $\{\varepsilon_n\}$ be a sequence of i.i.d random variables taking values in $1$ and $-1$ with probability $\frac{1}{2}$.
  Then $f_{\varepsilon}$ is a continuous function with probability $1$.
\end{theorem}

To prove the theorem, we will need several lemmas.

\begin{lemma}
  \label{lem:main-technical-lemma}
  Let $N \in \N$.
  Then for any $\{a_n\}_{n=-N}^N$, the following probability bound holds.
  \begin{align*}
    \P\left( \norm{ \sum_{n=-N}^N \varepsilon_n a_n e_n }_{\infty} >  C \sqrt{\log(N)} \left( \sum_{n=-N}^N |a_n|^2 \right)^{\frac{1}{2}} \right) \leq \frac{1}{N^2}
  \end{align*}
  Here the constant $C$ is absolute, i.e. independent of $N$.
\end{lemma}
\begin{proof}
  The first step is to consider the maximum, not over all of $[0,1]$, but over the points $\left\{ \frac{j}{N^2} \right\}_{j = 0}^N$.
  It will suffice to bound the probability for any given point by $\frac{1}{N^4}$, and then use the union bound to get the desired inequality.
  Since $\varepsilon_n$ is a subgaussian random variable with mean $0$, by Hoeffding-Chernoff-Azuma inequality, we get the following bound for any $t \in [0,1]$.
  \begin{align*}
    \P\left( \norm{ \sum_{n=-N}^N \varepsilon_n a_n e_n }_{\infty} >  C \sqrt{\log(n)} \left( \sum_{n=-N}^N |a_n|^2 \right)^{\frac{1}{2}} \right) \leq C^{\prime} \exp(-cC^2 \log(n))
  \end{align*}
  We can pick a $C$ large enough so that the right hand side is bounded above by $\frac{1}{N^4}$, and that concludes the first step after we use the union bound.

  The next step is to extend the argument to all of $[0,1]$.
  The key trick here will be to estimate the maximum value of trigonometric polynomials away from points of the form $\frac{j}{N^2}$ using the maximum we derived in step $1$.
  Bernstein's inequality for trigonometric polynomial helps in this regard: given a trigonometric polynomial $p$ of degree $n$, the following inequality relates the $\norm{\cdot}_{\infty}$-norm of $p^{\prime}$ and $p$.
  \begin{align*}
    \norm{p^{\prime}}_{\infty} \leq n \norm{p}_{\infty}
  \end{align*}
  Let $V$ be the maximum value of the trigonometric polynomial $p = \sum_{n=-N}^N \varepsilon_n a_n e_n$ achieves on points of the form $\frac{j}{N^2}$, and let $W$ be the maximum value over all of $[0,1]$, and say it is achieved at some point $t$, and let $s$ be the closest point of the form $\frac{j}{N^2}$.
  Then, by the mean value theorem, we get the following relation between $V$ and $W$.
  \begin{align*}
    W &\leq V + \norm{p^{\prime}}_{\infty} |t-s| \\
      &\leq V + \frac{N \norm{p}_{\infty}}{N^2} \\
      &= V + \frac{W}{N}
  \end{align*}
  This means for $N > 1$, $W \leq 2V$, and this proves the lemma.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:continuous-l2-function}]
  For $M \in \N$, define the function $f_{M, \varepsilon}$ in the following manner.
  \begin{align*}
    f_{M, \varepsilon} = \sum_{2^M \leq |n| < 2^{M+1}} \varepsilon_n \widehat{f}(n) e_n
  \end{align*}
  We now use Lemma \ref{lem:main-technical-lemma} with $N = 2^{M+1}$, $a_n = 0$ for $|n| < 2^M$, and $a_n = \widehat{f}(n)$ for $2^M \leq n < 2^{M+1}$.
  \begin{align*}
    \P\left( \norm{f_{M,\varepsilon}}_{\infty} > C \sqrt{M} \norm{f_{M,\varepsilon}}_2 \right) < \frac{1}{2^{2(M+1)}}
  \end{align*}
  \confused{It seems something is going wrong here because the sum of probability upper bound converges too easily, which means one might be making a mistake, or strengthen the result significantly.}
  By the Borel-Cantelli lemma, for almost every $\varepsilon$, $\norm{f_{M, \varepsilon}}_{\infty}$ eventually becomes smaller than $C\sqrt{M} \norm{f_{M,\varepsilon}}_2$.

  Pick $\varepsilon$ to be one of the instances where the above described situation does happen.
  We have that $f$ is an infinite sum of continuous functions.
  \begin{align*}
    f = \varepsilon_0 \widehat{f}(0) e_0 + \sum_{M=0}^{\infty} f_{M, \varepsilon}
  \end{align*}
  This will converge to a continuous function if the sequence of partial sums is uniformly Cauchy.
  To see that is indeed the case, pick $K_1$ and $K_2$ larger than the threshold $M$ after which $\norm{f_{M, \varepsilon}} < C\sqrt{M}$.
  \begin{align*}
    \norm{f_{K_2, \varepsilon} - f_{K_1, \varepsilon}}_{\infty} &\leq \sum_{M=K_1}^{K_2} \norm{f_{M, \varepsilon}}_{\infty} \\
                                                                &\leq \sum_{M=K_1}^\infty C \sqrt{M} \norm{f_{M, \varepsilon}}_2 \\
                                                                &\leq C \left( \sum_{M=K_1}^{\infty} \left( \frac{1}{M} \right)^2 \right)^{\frac{1}{2}} \left( \sum_{M=K_1}^{\infty} M^3 \norm{f_{M, \varepsilon}}_2^2 \right)^{\frac{1}{2}} \\
                                                                &\leq \frac{C^{\prime}}{K_1} \left( \sum_{n \in \Z} (C^{\prime \prime} \log(|n| + 1))^3 \widehat{f}(n)^2 \right) \\
    &\leq \frac{C^{\prime\prime\prime}}{K_1}
  \end{align*}
  The upper bound goes to $0$ as $K_1$ goes to $\infty$, which shows the sequence is uniformly Cauchy, and thus the limit is a continuous function.
\end{proof}

We now prove a moment bound for sums of subgaussian random variables.
\begin{theorem}[Khinchin's inequality]
  Let $\{X_1, \ldots, X_n\}$ be i.i.d subgaussian random variables with $\E(X_i) = 0$ and $\E(X_i^2) = 1$.
  Then for any $p \in [1, \infty)$, there exist constants $A_p$ and $B_p$ greater than $0$ such that for any vector $\mathbf{a} \in \R^n$, the following moment bound holds.
  \begin{align*}
    A_p \norm{\mathbf{a}}_2 \leq \left( \E \left| \sum_{i=1}^n a_i X_i \right|^p \right)^{\frac{1}{p}} \leq  B_p \norm{\mathbf{a}}_2
  \end{align*}
\end{theorem}
\begin{proof}
  Consider first the case where $p > 2$.
  Then, using HÃ¶lder's inequality for the convex function $x \mapsto x^{\frac{p}{2}}$, we get the following.
  \begin{align*}
\left( \E \left| \sum_{i=1}^n a_i X_i \right|^2 \right)^{\frac{1}{2}} \leq \left( \E \left| \sum_{i=1}^n a_i X_i \right|^p \right)^{\frac{1}{p}}
  \end{align*}
  This shows that for $p > 2$, we can set $A_p = 1$.
  To get $B_p$, we use the moment condition on subgaussian random variables.
  Since the sum of subgaussian random variables is subgaussian, we have that $Y = \sum_{i=1}^n a_i X_i$ is subgaussian, and thus satisfies the moment bound.
  We have seen that the absolute constant $K$ will only depend on $\norm{\mathbf{a}}_2$, giving us the upper bound.
  \begin{align*}
    \E(|Y|^p)^{\frac{1}{p}} \leq K \sqrt{p} \norm{\mathbf{a}}_2
  \end{align*}
  Setting $B_p = K\sqrt{p}$ proves the result in this case.

  For $p < 2$, it will suffice to prove it for $p = 1$, since the $p$\textsuperscript{th} moment of $|Y|$ is an increasing function of $Y$ and bounded above by $\norm{\mathbf{a}}_2$ by HÃ¶lder's inequality, which means we can set $B_p = 1$ (or the previous argument will also work, but $B_p = 1$ is better than $B_p = K\sqrt{p}$).
  Thus we just need to show the lower bound for $p = 1$.
  In this case, the inequality follows from Cauchy-Schwartz.
  \begin{align*}
    \E(|Y|^2) &\leq \sqrt{\E(|Y|) \E(|Y|^3)}
  \end{align*}
  Using Khinchin's inequality for $p=3$, we deal with $\E(|Y|^3) \leq B_3 \norm{\mathbf{a}}_2$.
  Squaring both sides, we see that $A_1 = B_3^{-3}$ works, and the proof is complete.
\end{proof}
There is a far reaching generalization of Khinchin's inequality, due to Kahane.
\begin{theorem}[Kahane inequality]
  Let $X$ be a normed vector space, and let $\{\varepsilon_1, \ldots, \varepsilon_n\}$ be i.i.d. Rademacher random variables.
  For any $p \in [1, \infty)$, there exists $A_p$ and $B_p$ greater than $0$ such that for any $\{a_1, \ldots, a_n\}$ in $X$, the following holds.
  \begin{align*}
    A_p \left( \E \norm{\sum_{j=1}^n \varepsilon_j a_j}^2_X \right)^{\frac{1}{2}} \leq \left( \E \norm{\sum_{j=1}^n \varepsilon_j a_j}^p_X \right)^{\frac{1}{p}} \leq B_p \left( \E \norm{\sum_{j=1}^n \varepsilon_j a_j}^2_X \right)^{\frac{1}{2}}
  \end{align*}
\end{theorem}
The proof of this inequality requires more machinery than the previous result, so we'll defer the proof until we have developed the required tools.

Recall that we got strong tail decay for bounded random variables using Hoeffding-Chernoff-Azuma inequality, since they're subgaussian, but it turns out, we can do much better than that using boundedness.

\begin{theorem}[Bennett's inequality]
  \label{thm:bennett}
  Let $\{X_1, \ldots, X_n\}$ be i.i.d random variables satisfying the following properties.
  \begin{enumerate}[(i)]
  \item $\norm{X_j}_{\infty} \leq 1$.
  \item $\E(X_j) = 0$ and $\E(X_j^2) = \delta$.
  \end{enumerate}
  Then for any $\mathbf{a} \in \R^n$, we have the following tail decay estimate.
  \begin{align*}
    \P\left( \left| \sum_{j=1}^n a_j X_j \right| > t \right) \leq
    \begin{cases}
      2 \exp \left(- \frac{t^2}{2e\delta \norm{\mathbf{a}}_2^2} \right)\text{ for $t \leq t_{\ast}$} \\
      2 \exp \left( - \frac{t}{4 \norm{\mathbf{a}}_{\infty}} \cdot \log\left( \frac{t \norm{\mathbf{a}}_{\infty}}{\delta \norm{\mathbf{a}}_2^2} \right) \right)\text{ for $t > t_{\ast}$}
    \end{cases}
  \end{align*}
  Here $t_{\ast} = e\delta \norm{\mathbf{a}}_2^2$.
\end{theorem}
Before we start to prove the theorem, let's show why the described tail bound is a very natural upper bound to consider.
Consider $\mathbf{a} = (1, 1, \ldots, 1)$.
Then $\sum a_j X_j$ is approximately $\cN(0, \delta \norm{\mathbf{a}}^2_2) = \cN(0, \delta n)$.
The tail should then behave something like $\exp \left( - \frac{t^2}{2\delta n} \right)$, which is precisely the first case in the upper bound.
If $\delta$ is small, i.e. $\delta n$ is bounded above by some constant $\lambda$, then the central limit theorem asymptotic does not apply, but rather the Poisson limit theorem asymptotic applies.
\begin{align*}
  \P\left( \sum_{j=1}^n a_j X_j > t \right) &\sim \sum_{j=t}^{\infty} e^{-\lambda} \frac{\lambda^j}{j!} \\
                                            &\sim e^{-\lambda} \frac{\lambda^j}{j!} \\
                                            &\sim \exp\left( -t \log\left( \frac{t}{e\lambda} \right) \right)
\end{align*}
Contrast this with the second case of the tail in Bennett's inequality.

\begin{proof}[Proof of Theorem \ref{thm:bennett}]
  Without loss of generality, assume that $\norm{\mathbf{a}}_{\infty} = 1$.
  Set $Y = \sum_{j=1}^n a_j X_j$, and let $\lambda > 0$.
  We then estimate the Laplace transform of $Y$.
  \begin{align*}
    \E\left( \exp(\lambda Y) \right) &= \prod_{j=1}^n \E\left(\exp\left(\lambda a_j X_j \right) \right)
  \end{align*}
  We use an elementary inequality to estimate the Laplace transform.
  \begin{align*}
    e^x \leq 1 + x + \frac{x^2}{2} e^{|x|}
  \end{align*}
  Thus, we have the following.
  \begin{align*}
    \E\left( \exp\left( \lambda a_j X_j \right) \right) &\leq \E\left(1 + \lambda a_j X_j + \frac{\lambda^2 a_j^2 X_j^2}{2} \exp\left( \left| \lambda a_j X_j \right| \right)\right) \\
    &\leq 1 + 0 + \frac{\lambda^2 a_j^2 \delta}{2} e^{\lambda}
  \end{align*}
  Putting all of it back together, we have the following inequality.
  \begin{align*}
    \E\left( \exp\left( \lambda Y \right) \right) &\leq \prod_{j=1}^n \left( 1 + \frac{\lambda^2 a_j^2 \delta e^{\lambda}}{2} \right) \\
                                                  &\leq \prod_{j=1}^n \exp\left( \frac{\lambda^2 a_j^2 \delta e^{\lambda}}{2} \right) \\
                                                  &= \exp\left( \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)
  \end{align*}
  Now that we have the Laplace transform estimate, we can use it to estimate tail probabilities.
  \begin{align*}
    \P(Y > t) &= \P(\exp(\lambda Y) > e^{\lambda t}) \\
              &\leq \frac{\exp\left( \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)}{e^{\lambda t}} \\
              &= \exp\left( -\lambda t + \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)
  \end{align*}

  The next step is to optimize this inequality as we vary $\lambda$.
  We begin by optimizing in the region $\lambda \leq 1$.
  In this case, the optimum $\lambda$ is $\frac{t}{e\delta \norm{\mathbf{a}_2^2}}$.
  Plugging in this value of $\lambda$ gives the first case of the upper bound, and this is valid for $t \leq e\delta \norm{\mathbf{a}}_2^2 = t_{\ast}$.

  In the region $\lambda > 1$, we use the inequality $\lambda \leq e^{\lambda}$.
  \begin{align*}
    \P(Y > t) &\leq \exp\left( -\lambda t + \frac{\delta\lambda^2 e^{\lambda}}{2} \norm{\mathbf{a}}_2^2 \right) \\
              &\leq \exp\left( -\lambda t + \frac{\delta\lambda e^{2\lambda}}{2} \norm{\mathbf{a}}_2^2 \right)
  \end{align*}
  Choose $\lambda$ such that $\lambda \norm{\mathbf{a}}_2^2 e^{2\lambda} = t$. Plugging that value in, we get the second case.

  Finally, doing this for $-t$ gives similar bounds, and we combine the two using union bound to get the claimed result. This completes the proof.
\end{proof}

% \begin{remark}
%   The convergence requirement in \eqref{eq:convergence-condition-example} can be relaxed significantly.
%   We chose to prove the theorem with exponent equal to $5$ to make the proof simpler.
% \end{remark}

% \include{Notation}

% \bibliography{Moduli}

\end{document}
